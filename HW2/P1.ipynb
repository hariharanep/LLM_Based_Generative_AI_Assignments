{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:08:48.124884Z",
     "iopub.status.busy": "2025-10-21T02:08:48.124649Z",
     "iopub.status.idle": "2025-10-21T02:08:52.214188Z",
     "shell.execute_reply": "2025-10-21T02:08:52.213430Z",
     "shell.execute_reply.started": "2025-10-21T02:08:48.124859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Installation\n",
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "Number of GPUs Available: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nI'm executing everything on a Kaggle notebook which comes with PyTorch automatically installed and 2 NVIDIA T4 GPUs that I can make use of for free. As a result there's not really any complex configurations, setup steps, or hardware specifications that I have to worry about or document.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup and Implementation Part 1\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"PyTorch Installation\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs Available: {num_gpus}\")\n",
    "else:\n",
    "    print(\"No GPUs available!\")\n",
    "\"\"\"\n",
    "I'm executing everything on a Kaggle notebook which comes with PyTorch automatically installed and 2 NVIDIA T4 GPUs that I can make use of for free. As a result there's not really any complex configurations, setup steps, or hardware specifications that I have to worry about or document.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:09:44.094190Z",
     "iopub.status.busy": "2025-10-21T02:09:44.093668Z",
     "iopub.status.idle": "2025-10-21T02:09:44.241272Z",
     "shell.execute_reply": "2025-10-21T02:09:44.240240Z",
     "shell.execute_reply.started": "2025-10-21T02:09:44.094169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Importing all the required libraries/dependencies\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from dataclasses import dataclass\n",
      "import time\n",
      "import os\n",
      "import argparse\n",
      "import torch.distributed as dist\n",
      "from torch.distributed.pipelining import pipeline, SplitPoint, ScheduleGPipe, Schedule1F1B, ScheduleInterleaved1F1B\n",
      "\n",
      "# global variables taken from the tutorial code\n",
      "global rank, device, pp_group, stage_index, num_stages, start_time, end_time\n",
      "\n",
      "# init_distributed method is the exact same as the tutorial code\n",
      "def init_distributed():\n",
      "   global rank, device, pp_group, stage_index, num_stages\n",
      "\n",
      "   rank = int(os.environ[\"LOCAL_RANK\"])\n",
      "   world_size = int(os.environ[\"WORLD_SIZE\"])\n",
      "   device = torch.device(f\"cuda:{rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
      "   dist.init_process_group()\n",
      "\n",
      "   pp_group = dist.new_group()\n",
      "   stage_index = rank\n",
      "   num_stages = world_size\n",
      "\n",
      "# ModelArgs class is the exact same as the tutorial code with n_processes(number of processes) attribute added to it\n",
      "@dataclass\n",
      "class ModelArgs:\n",
      "   dim: int = 504\n",
      "   n_layers: int = 8\n",
      "   n_heads: int = 8\n",
      "   vocab_size: int = 10000\n",
      "   n_processes: int = 2\n",
      "\n",
      "# Transformer Model class is the exact same as the tutorial code\n",
      "class Transformer(nn.Module):\n",
      "   def __init__(self, model_args: ModelArgs):\n",
      "      super().__init__()\n",
      "\n",
      "      self.tok_embeddings = nn.Embedding(model_args.vocab_size, model_args.dim)\n",
      "\n",
      "      self.layers = torch.nn.ModuleDict()\n",
      "      for layer_id in range(model_args.n_layers):\n",
      "            self.layers[str(layer_id)] = nn.TransformerDecoderLayer(model_args.dim, model_args.n_heads, batch_first=True)\n",
      "\n",
      "      self.norm = nn.LayerNorm(model_args.dim)\n",
      "      self.output = nn.Linear(model_args.dim, model_args.vocab_size)\n",
      "\n",
      "   def forward(self, tokens: torch.Tensor):\n",
      "      h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens\n",
      "\n",
      "      for layer in self.layers.values():\n",
      "            h = layer(h, h)\n",
      "\n",
      "      h = self.norm(h) if self.norm else h\n",
      "      output = self.output(h).clone() if self.output else h\n",
      "      return output\n",
      "\n",
      "# This tracer_model_split function splits the model into stages depending on the number of processes\n",
      "def tracer_model_split(model, example_input, num_processes):\n",
      "    split_spec = {}\n",
      "    \n",
      "    if num_processes == 2:\n",
      "        # Split model into two stages\n",
      "        # First Stage: token embeddings layers plus first half of the TransformerDecoder layers\n",
      "        # Second Stage: second half of the TransformerDecoder layers plus normalization and output layer\n",
      "        halfway = model_args.n_layers // 2\n",
      "        split_spec = {\n",
      "            f\"layers.{halfway}\": SplitPoint.BEGINNING\n",
      "        }        \n",
      "    elif num_processes == 4:\n",
      "        # Split model into four stages\n",
      "        # First Stage: token embeddings layers plus first quarter of the TransformerDecoder layers\n",
      "        # Second Stage: second quarter of the TransformerDecoder layers\n",
      "        # Third Stage: third quarter of the TransformerDecoder layers\n",
      "        # Fourth Stage: last quarter of the TransformerDecoder layers plus normalization and output layer\n",
      "        split = model_args.n_layers // 4\n",
      "        split_spec = {\n",
      "            f\"layers.{split}\": SplitPoint.BEGINNING,      \n",
      "            f\"layers.{2*split}\": SplitPoint.BEGINNING,   \n",
      "            f\"layers.{3*split}\": SplitPoint.BEGINNING\n",
      "        }\n",
      "\n",
      "    # Returns stage in this pipeline that corresponds to the current stage_index    \n",
      "    stage = pipeline(\n",
      "        model,\n",
      "        mb_args=(example_input,),  \n",
      "        split_spec=split_spec,\n",
      "    ).build_stage(\n",
      "        stage_index,\n",
      "        device,\n",
      "    )    \n",
      "    return stage\n",
      "\n",
      "# This create_interleaved_stages_tracer function splits the model into interleaved stages for Interleaved1F1B scheduling\n",
      "# num_chunks represents the number of chunks/stages per rank\n",
      "def create_interleaved_stages_tracer(model, example_input, num_processes, num_chunks):\n",
      "    # Total chunks in total\n",
      "    total_chunks = num_processes * num_chunks\n",
      "    # Total number of model layers each chunk should store\n",
      "    layers_per_chunk = model_args.n_layers // total_chunks\n",
      "    \n",
      "    # Generate all the split points for each chunk\n",
      "    split_spec = {}\n",
      "    for i in range(1, total_chunks):\n",
      "        split_layer = i * layers_per_chunk\n",
      "        split_spec[f\"layers.{split_layer}\"] = SplitPoint.BEGINNING\n",
      "\n",
      "    # Create pipeline for Interleaved1F1B scheduling\n",
      "    pipe = pipeline(\n",
      "        model,\n",
      "        mb_args=(example_input,),\n",
      "        split_spec=split_spec,\n",
      "    )\n",
      "\n",
      "    \"\"\"\n",
      "    Interleaves the stages and returns the result as a list of stages for this rank\n",
      "    Example for num_processes = 2, num_chunks = 2, 4 total stages(Stage 0, Stage 1, Stage 2, Stage 3)\n",
      "    Rank = 0, first process\n",
      "    chunk_idx = 0: global_stage_idx = 0 + (0 * 2) = 0 = Stage 0\n",
      "    chunk_idx = 1: global_stage_idx = 0 + (1 * 2) = 2 = Stage 2\n",
      "\n",
      "    Rank = 1, second process\n",
      "    chunk_idx = 0: global_stage_idx = 1 + (0 * 2) = 1 = Stage 1\n",
      "    chunk_idx = 1: global_stage_idx = 1 + (1 * 2) = 3 = Stage 3\n",
      "    \"\"\"    \n",
      "    stages = []\n",
      "    for chunk_idx in range(num_chunks):\n",
      "        global_stage_idx = rank + (chunk_idx * num_processes)\n",
      "                \n",
      "        stage = pipe.build_stage(global_stage_idx, device)\n",
      "        stages.append(stage)\n",
      "    \n",
      "    # Return list of stages that correspond to the current rank\n",
      "    return stages\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # same as tutorial code\n",
      "    init_distributed()\n",
      "\n",
      "    # parse the additional model and distributed parallelism arguments provided in command line\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\"--layers\", type=int)\n",
      "    parser.add_argument(\"--attention_heads\", type=int)\n",
      "    parser.add_argument(\"--processes\", type=int)\n",
      "    parser.add_argument(\"--schedule\", type=str)\n",
      "    # num_chunks will always be passed in as half of layers(total number of TransformerDecoder layers) to distribute the workload evenly across all ranks/GPUs\n",
      "    parser.add_argument(\"--num_chunks\", type=int, default=2)\n",
      "\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    # Set model arguments per the model and distributed parallelism arguments provided in command line\n",
      "    model_args = ModelArgs()\n",
      "    model_args.n_layers = args.layers\n",
      "    model_args.n_heads = args.attention_heads\n",
      "    model_args.n_processes = args.processes\n",
      "    num_processes = args.processes\n",
      "\n",
      "    # Create model\n",
      "    model = Transformer(model_args=model_args)\n",
      "\n",
      "    # Random singular batch of dummy data\n",
      "    x = torch.ones(32, 500, dtype=torch.long)\n",
      "    y = torch.randint(0, model_args.vocab_size, (32, 500), dtype=torch.long)\n",
      "\n",
      "    # Splitting dummy data into 4 micromatches\n",
      "    num_microbatches = 4\n",
      "    example_input_microbatch = x.chunk(num_microbatches)[0]\n",
      "\n",
      "    # use tracer_model_split to get stage for GPipe and 1F1B schedules\n",
      "    stage = None\n",
      "    if args.schedule in [\"GPipe\", \"1F1B\"]:\n",
      "        stage = tracer_model_split(model, example_input_microbatch, num_processes)\n",
      "        stages = None\n",
      "    # use create_interleaved_stages_tracer to get list of stages for this rank for Interleaved1F1B schedule\n",
      "    elif args.schedule == \"Interleaved1F1B\":\n",
      "        stages = create_interleaved_stages_tracer(model, example_input_microbatch, num_processes, args.num_chunks)\n",
      "        stage = None\n",
      "\n",
      "    # Move data to device\n",
      "    x = x.to(device)\n",
      "    y = y.to(device)\n",
      "\n",
      "    # tokenwise_loss_fn taken from tutorial code\n",
      "    def tokenwise_loss_fn(outputs, targets):\n",
      "        loss_fn = nn.CrossEntropyLoss()\n",
      "        outputs = outputs.reshape(-1, model_args.vocab_size)\n",
      "        targets = targets.reshape(-1)\n",
      "        return loss_fn(outputs, targets)\n",
      "\n",
      "    schedule = None\n",
      "    optimizer = None\n",
      "    # Create schedule and Adam optimizer with stage or stages list created previously\n",
      "    # GPipe and 1F1B schedules are 1 stage per rank\n",
      "    # Interleaved1F1B schedule is multiple stages per rank hence why its handled differently\n",
      "    if args.schedule == \"GPipe\":\n",
      "        schedule = ScheduleGPipe(stage, n_microbatches=num_microbatches, loss_fn=tokenwise_loss_fn)\n",
      "        optimizer = torch.optim.Adam(stage.submod.parameters(), lr=0.001)\n",
      "    elif args.schedule == \"1F1B\":\n",
      "        schedule = Schedule1F1B(stage, n_microbatches=num_microbatches, loss_fn=tokenwise_loss_fn)\n",
      "        optimizer = torch.optim.Adam(stage.submod.parameters(), lr=0.001)\n",
      "    elif args.schedule == \"Interleaved1F1B\":\n",
      "        schedule = ScheduleInterleaved1F1B(stages, n_microbatches=num_microbatches, loss_fn=tokenwise_loss_fn)\n",
      "        params = []\n",
      "        for s in stages:\n",
      "            params.extend(s.submod.parameters())\n",
      "        optimizer = torch.optim.Adam(params, lr=0.001)\n",
      "    \n",
      "    num_epochs = 3\n",
      "\n",
      "    # Start tracking start time and ending time at one specific rank to avoid invoking  start_time = time.time() for each rank in the beginning\n",
      "    if rank == num_processes - 1:\n",
      "        global start_time\n",
      "        start_time = time.time()\n",
      "    \n",
      "    # Train dummy data for 3 epochs\n",
      "    for epoch in range(num_epochs):\n",
      "        print(f\"Rank: {rank}, Starting Epoch: {epoch + 1}\", flush=True)\n",
      "\n",
      "        # clears the gradients of all parameters that the optimizer is currently tracking\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # If rank = last rank in schedule, print the avg loss\n",
      "        if rank == num_processes - 1:\n",
      "            losses = []\n",
      "            output = schedule.step(x, target=y, losses=losses)\n",
      "            avg_loss = sum(losses) / len(losses)\n",
      "            print(f\"Rank: {rank}, Avg Loss: {avg_loss:.4f}\", flush=True)\n",
      "        # Execute stage normally\n",
      "        else:\n",
      "            schedule.step(x)\n",
      "        \n",
      "        # update model parameters\n",
      "        optimizer.step()\n",
      "    \n",
      "    \n",
      "    # End tracking for total training time, record total training throughput and approximate end to end training time\n",
      "    if rank == num_processes - 1:\n",
      "        global end_time\n",
      "        end_time = time.time()\n",
      "        total_time = end_time - start_time\n",
      "        # Dummy data batch = (32 rows, 500 tokens each row)\n",
      "        # Trained 3 epochs(went through whole batch 3 times)\n",
      "        # Total throughput = (32 x 500 x 3) / total end to end training time\n",
      "        print(f\"Total Training Throughput: {((32 * 500 * 3) / total_time):.3f}\")\n",
      "        print(f\"End To End Training Time: {(total_time):.3f}\")\n",
      "    \n",
      "    # cleans up and terminates previously initialized distributed process group\n",
      "    dist.destroy_process_group()\n"
     ]
    }
   ],
   "source": [
    "# Setup and Implementation Part 2\n",
    "# distributed_training.py = code that implements a transformer model with pipeline parallelism support \n",
    "!cat ../input/temp65/distributed_training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:11:24.377997Z",
     "iopub.status.busy": "2025-10-21T02:11:24.377629Z",
     "iopub.status.idle": "2025-10-21T02:11:24.384304Z",
     "shell.execute_reply": "2025-10-21T02:11:24.383389Z",
     "shell.execute_reply.started": "2025-10-21T02:11:24.377970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Only able to present results where Number of processes = 2 since I only have 2 GPUs and if I execute everything with just CPUs, the whole program execution starts hanging and never finishes.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experimental Evaluation Part 1\n",
    "\"Only able to present results where Number of processes = 2 since I only have 2 GPUs and if I execute everything with just CPUs, the whole program execution starts hanging and never finishes.\"\n",
    "# GPipe Scheduling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:11:30.653672Z",
     "iopub.status.busy": "2025-10-21T02:11:30.652873Z",
     "iopub.status.idle": "2025-10-21T02:11:44.431867Z",
     "shell.execute_reply": "2025-10-21T02:11:44.430938Z",
     "shell.execute_reply.started": "2025-10-21T02:11:30.653640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:11:32.477000 79 torch/distributed/run.py:792] \n",
      "W1021 02:11:32.477000 79 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:11:32.477000 79 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:11:32.477000 79 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3813\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.2788\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0630\n",
      "Total Training Throughput: 15281.787\n",
      "End To End Training Time: 3.141\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 4 --attention_heads 4 --schedule GPipe --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:11:44.433609Z",
     "iopub.status.busy": "2025-10-21T02:11:44.433368Z",
     "iopub.status.idle": "2025-10-21T02:11:55.205428Z",
     "shell.execute_reply": "2025-10-21T02:11:55.204623Z",
     "shell.execute_reply.started": "2025-10-21T02:11:44.433588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:11:46.172000 117 torch/distributed/run.py:792] \n",
      "W1021 02:11:46.172000 117 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:11:46.172000 117 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:11:46.172000 117 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3771\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.2850\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0615\n",
      "Total Training Throughput: 17518.957\n",
      "End To End Training Time: 2.740\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 4 --attention_heads 8 --schedule GPipe --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:11:55.206646Z",
     "iopub.status.busy": "2025-10-21T02:11:55.206427Z",
     "iopub.status.idle": "2025-10-21T02:12:05.951288Z",
     "shell.execute_reply": "2025-10-21T02:12:05.950537Z",
     "shell.execute_reply.started": "2025-10-21T02:11:55.206624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:11:56.910000 155 torch/distributed/run.py:792] \n",
      "W1021 02:11:56.910000 155 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:11:56.910000 155 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:11:56.910000 155 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3792\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.2898\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0686\n",
      "Total Training Throughput: 17651.923\n",
      "End To End Training Time: 2.719\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 4 --attention_heads 12 --schedule GPipe --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:12:05.953663Z",
     "iopub.status.busy": "2025-10-21T02:12:05.953421Z",
     "iopub.status.idle": "2025-10-21T02:12:18.819760Z",
     "shell.execute_reply": "2025-10-21T02:12:18.819017Z",
     "shell.execute_reply.started": "2025-10-21T02:12:05.953642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:12:07.679000 193 torch/distributed/run.py:792] \n",
      "W1021 02:12:07.679000 193 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:12:07.679000 193 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:12:07.679000 193 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3797\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3120\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0902\n",
      "Total Training Throughput: 12171.319\n",
      "End To End Training Time: 3.944\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 8 --attention_heads 4 --schedule GPipe --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:12:18.821046Z",
     "iopub.status.busy": "2025-10-21T02:12:18.820721Z",
     "iopub.status.idle": "2025-10-21T02:12:32.234914Z",
     "shell.execute_reply": "2025-10-21T02:12:32.234158Z",
     "shell.execute_reply.started": "2025-10-21T02:12:18.821010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:12:20.497000 231 torch/distributed/run.py:792] \n",
      "W1021 02:12:20.497000 231 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:12:20.497000 231 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:12:20.497000 231 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3679\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3177\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0854\n",
      "Total Training Throughput: 11738.217\n",
      "End To End Training Time: 4.089\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 8 --attention_heads 8 --schedule GPipe --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:12:32.236195Z",
     "iopub.status.busy": "2025-10-21T02:12:32.235932Z",
     "iopub.status.idle": "2025-10-21T02:12:45.802409Z",
     "shell.execute_reply": "2025-10-21T02:12:45.801544Z",
     "shell.execute_reply.started": "2025-10-21T02:12:32.236172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:12:33.929000 269 torch/distributed/run.py:792] \n",
      "W1021 02:12:33.929000 269 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:12:33.929000 269 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:12:33.929000 269 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3757\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.2920\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0721\n",
      "Total Training Throughput: 11286.389\n",
      "End To End Training Time: 4.253\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 8 --attention_heads 12 --schedule GPipe --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:12:45.803581Z",
     "iopub.status.busy": "2025-10-21T02:12:45.803309Z",
     "iopub.status.idle": "2025-10-21T02:13:01.733981Z",
     "shell.execute_reply": "2025-10-21T02:13:01.733268Z",
     "shell.execute_reply.started": "2025-10-21T02:12:45.803558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:12:47.585000 307 torch/distributed/run.py:792] \n",
      "W1021 02:12:47.585000 307 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:12:47.585000 307 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:12:47.585000 307 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3838\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3220\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0916\n",
      "Total Training Throughput: 9089.485\n",
      "End To End Training Time: 5.281\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 12 --attention_heads 4 --schedule GPipe --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:13:01.735189Z",
     "iopub.status.busy": "2025-10-21T02:13:01.734958Z",
     "iopub.status.idle": "2025-10-21T02:13:17.664531Z",
     "shell.execute_reply": "2025-10-21T02:13:17.663761Z",
     "shell.execute_reply.started": "2025-10-21T02:13:01.735159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:13:03.411000 345 torch/distributed/run.py:792] \n",
      "W1021 02:13:03.411000 345 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:13:03.411000 345 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:13:03.411000 345 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3737\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3002\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0838\n",
      "Total Training Throughput: 8906.597\n",
      "End To End Training Time: 5.389\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 12 --attention_heads 8 --schedule GPipe --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:13:17.667297Z",
     "iopub.status.busy": "2025-10-21T02:13:17.667048Z",
     "iopub.status.idle": "2025-10-21T02:13:34.107544Z",
     "shell.execute_reply": "2025-10-21T02:13:34.106787Z",
     "shell.execute_reply.started": "2025-10-21T02:13:17.667276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:13:19.360000 383 torch/distributed/run.py:792] \n",
      "W1021 02:13:19.360000 383 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:13:19.360000 383 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:13:19.360000 383 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3715\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3142\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0820\n",
      "Total Training Throughput: 8216.436\n",
      "End To End Training Time: 5.842\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 12 --attention_heads 12 --schedule GPipe --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:13:34.108648Z",
     "iopub.status.busy": "2025-10-21T02:13:34.108444Z",
     "iopub.status.idle": "2025-10-21T02:13:34.113989Z",
     "shell.execute_reply": "2025-10-21T02:13:34.113376Z",
     "shell.execute_reply.started": "2025-10-21T02:13:34.108629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Only able to present results where Number of processes = 2 since I only have 2 GPUs and if I execute everything with just CPUs, the whole program execution starts hanging and never finishes.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experimental Evaluation Part 1\n",
    "\"Only able to present results where Number of processes = 2 since I only have 2 GPUs and if I execute everything with just CPUs, the whole program execution starts hanging and never finishes.\"\n",
    "# 1F1B Scheduling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:13:34.115053Z",
     "iopub.status.busy": "2025-10-21T02:13:34.114829Z",
     "iopub.status.idle": "2025-10-21T02:13:44.847427Z",
     "shell.execute_reply": "2025-10-21T02:13:44.846680Z",
     "shell.execute_reply.started": "2025-10-21T02:13:34.115033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:13:35.809000 421 torch/distributed/run.py:792] \n",
      "W1021 02:13:35.809000 421 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:13:35.809000 421 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:13:35.809000 421 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3778\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.2965\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0689\n",
      "Total Training Throughput: 17685.948\n",
      "End To End Training Time: 2.714\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 4 --attention_heads 4 --schedule 1F1B --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:13:44.848691Z",
     "iopub.status.busy": "2025-10-21T02:13:44.848459Z",
     "iopub.status.idle": "2025-10-21T02:13:55.591095Z",
     "shell.execute_reply": "2025-10-21T02:13:55.590371Z",
     "shell.execute_reply.started": "2025-10-21T02:13:44.848662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:13:46.552000 459 torch/distributed/run.py:792] \n",
      "W1021 02:13:46.552000 459 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:13:46.552000 459 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:13:46.552000 459 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3763\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.2950\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0704\n",
      "Total Training Throughput: 17387.224\n",
      "End To End Training Time: 2.761\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 4 --attention_heads 8 --schedule 1F1B --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:13:55.592331Z",
     "iopub.status.busy": "2025-10-21T02:13:55.592064Z",
     "iopub.status.idle": "2025-10-21T02:14:06.246539Z",
     "shell.execute_reply": "2025-10-21T02:14:06.245696Z",
     "shell.execute_reply.started": "2025-10-21T02:13:55.592307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:13:57.300000 497 torch/distributed/run.py:792] \n",
      "W1021 02:13:57.300000 497 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:13:57.300000 497 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:13:57.300000 497 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3659\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.2858\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0689\n",
      "Total Training Throughput: 16730.582\n",
      "End To End Training Time: 2.869\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 4 --attention_heads 12 --schedule 1F1B --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:14:06.247777Z",
     "iopub.status.busy": "2025-10-21T02:14:06.247532Z",
     "iopub.status.idle": "2025-10-21T02:14:19.640484Z",
     "shell.execute_reply": "2025-10-21T02:14:19.639556Z",
     "shell.execute_reply.started": "2025-10-21T02:14:06.247754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:14:08.005000 535 torch/distributed/run.py:792] \n",
      "W1021 02:14:08.005000 535 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:14:08.005000 535 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:14:08.005000 535 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3806\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3105\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0844\n",
      "Total Training Throughput: 11778.643\n",
      "End To End Training Time: 4.075\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 8 --attention_heads 4 --schedule 1F1B --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:14:19.641997Z",
     "iopub.status.busy": "2025-10-21T02:14:19.641749Z",
     "iopub.status.idle": "2025-10-21T02:14:33.061079Z",
     "shell.execute_reply": "2025-10-21T02:14:33.060346Z",
     "shell.execute_reply.started": "2025-10-21T02:14:19.641974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:14:21.318000 573 torch/distributed/run.py:792] \n",
      "W1021 02:14:21.318000 573 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:14:21.318000 573 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:14:21.318000 573 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3787\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3107\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0862\n",
      "Total Training Throughput: 11584.665\n",
      "End To End Training Time: 4.143\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 8 --attention_heads 8 --schedule 1F1B --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:14:33.062351Z",
     "iopub.status.busy": "2025-10-21T02:14:33.062102Z",
     "iopub.status.idle": "2025-10-21T02:14:46.601207Z",
     "shell.execute_reply": "2025-10-21T02:14:46.600435Z",
     "shell.execute_reply.started": "2025-10-21T02:14:33.062328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:14:34.750000 611 torch/distributed/run.py:792] \n",
      "W1021 02:14:34.750000 611 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:14:34.750000 611 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:14:34.750000 611 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3776\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3059\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0772\n",
      "Total Training Throughput: 10936.940\n",
      "End To End Training Time: 4.389\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 8 --attention_heads 12 --schedule 1F1B --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:14:46.602951Z",
     "iopub.status.busy": "2025-10-21T02:14:46.602518Z",
     "iopub.status.idle": "2025-10-21T02:15:02.995180Z",
     "shell.execute_reply": "2025-10-21T02:15:02.994274Z",
     "shell.execute_reply.started": "2025-10-21T02:14:46.602916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:14:48.334000 649 torch/distributed/run.py:792] \n",
      "W1021 02:14:48.334000 649 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:14:48.334000 649 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:14:48.334000 649 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3918\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3003\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0843\n",
      "Total Training Throughput: 8666.590\n",
      "End To End Training Time: 5.539\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 12 --attention_heads 4 --schedule 1F1B --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:15:02.996559Z",
     "iopub.status.busy": "2025-10-21T02:15:02.996311Z",
     "iopub.status.idle": "2025-10-21T02:15:19.369740Z",
     "shell.execute_reply": "2025-10-21T02:15:19.368809Z",
     "shell.execute_reply.started": "2025-10-21T02:15:02.996537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:15:04.724000 687 torch/distributed/run.py:792] \n",
      "W1021 02:15:04.724000 687 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:15:04.724000 687 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:15:04.724000 687 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3752\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3200\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0951\n",
      "Total Training Throughput: 8664.951\n",
      "End To End Training Time: 5.540\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 12 --attention_heads 8 --schedule 1F1B --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:15:19.371213Z",
     "iopub.status.busy": "2025-10-21T02:15:19.370958Z",
     "iopub.status.idle": "2025-10-21T02:15:35.791411Z",
     "shell.execute_reply": "2025-10-21T02:15:35.790668Z",
     "shell.execute_reply.started": "2025-10-21T02:15:19.371190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:15:21.045000 725 torch/distributed/run.py:792] \n",
      "W1021 02:15:21.045000 725 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:15:21.045000 725 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:15:21.045000 725 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3766\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3104\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0871\n",
      "Total Training Throughput: 8056.898\n",
      "End To End Training Time: 5.958\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 12 --attention_heads 12 --schedule 1F1B --processes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:15:35.794988Z",
     "iopub.status.busy": "2025-10-21T02:15:35.794777Z",
     "iopub.status.idle": "2025-10-21T02:15:35.800363Z",
     "shell.execute_reply": "2025-10-21T02:15:35.799755Z",
     "shell.execute_reply.started": "2025-10-21T02:15:35.794967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Only able to present results where Number of processes = 2 since I only have 2 GPUs and if I execute everything with just CPUs, the whole program execution starts hanging and never finishes.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experimental Evaluation Part 1\n",
    "\"Only able to present results where Number of processes = 2 since I only have 2 GPUs and if I execute everything with just CPUs, the whole program execution starts hanging and never finishes.\"\n",
    "# Interleaved1F1B Scheduling Results\n",
    "# num_chunks will always be passed in as half of layers(total number of TransformerDecoder layers) to evenly distribute the workload across both GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:15:35.801263Z",
     "iopub.status.busy": "2025-10-21T02:15:35.801075Z",
     "iopub.status.idle": "2025-10-21T02:15:46.637266Z",
     "shell.execute_reply": "2025-10-21T02:15:46.636500Z",
     "shell.execute_reply.started": "2025-10-21T02:15:35.801244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:15:37.580000 763 torch/distributed/run.py:792] \n",
      "W1021 02:15:37.580000 763 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:15:37.580000 763 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:15:37.580000 763 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3784\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.2736\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0630\n",
      "Total Training Throughput: 17179.105\n",
      "End To End Training Time: 2.794\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 4 --attention_heads 4 --schedule Interleaved1F1B --processes 2 --num_chunks 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:15:46.638494Z",
     "iopub.status.busy": "2025-10-21T02:15:46.638263Z",
     "iopub.status.idle": "2025-10-21T02:15:57.316903Z",
     "shell.execute_reply": "2025-10-21T02:15:57.315794Z",
     "shell.execute_reply.started": "2025-10-21T02:15:46.638472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:15:48.382000 801 torch/distributed/run.py:792] \n",
      "W1021 02:15:48.382000 801 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:15:48.382000 801 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:15:48.382000 801 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3793\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.2828\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0629\n",
      "Total Training Throughput: 17085.271\n",
      "End To End Training Time: 2.809\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 4 --attention_heads 8 --schedule Interleaved1F1B --processes 2 --num_chunks 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:15:57.318962Z",
     "iopub.status.busy": "2025-10-21T02:15:57.318678Z",
     "iopub.status.idle": "2025-10-21T02:16:08.073869Z",
     "shell.execute_reply": "2025-10-21T02:16:08.072961Z",
     "shell.execute_reply.started": "2025-10-21T02:15:57.318933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:15:59.038000 839 torch/distributed/run.py:792] \n",
      "W1021 02:15:59.038000 839 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:15:59.038000 839 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:15:59.038000 839 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3802\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.2806\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0609\n",
      "Total Training Throughput: 16215.554\n",
      "End To End Training Time: 2.960\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 4 --attention_heads 12 --schedule Interleaved1F1B --processes 2 --num_chunks 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:16:08.075290Z",
     "iopub.status.busy": "2025-10-21T02:16:08.074973Z",
     "iopub.status.idle": "2025-10-21T02:16:21.844590Z",
     "shell.execute_reply": "2025-10-21T02:16:21.843810Z",
     "shell.execute_reply.started": "2025-10-21T02:16:08.075258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:16:09.800000 877 torch/distributed/run.py:792] \n",
      "W1021 02:16:09.800000 877 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:16:09.800000 877 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:16:09.800000 877 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3748\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3045\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0773\n",
      "Total Training Throughput: 11887.872\n",
      "End To End Training Time: 4.038\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 8 --attention_heads 4 --schedule Interleaved1F1B --processes 2 --num_chunks 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:16:21.845865Z",
     "iopub.status.busy": "2025-10-21T02:16:21.845587Z",
     "iopub.status.idle": "2025-10-21T02:16:35.615667Z",
     "shell.execute_reply": "2025-10-21T02:16:35.614907Z",
     "shell.execute_reply.started": "2025-10-21T02:16:21.845832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:16:23.560000 915 torch/distributed/run.py:792] \n",
      "W1021 02:16:23.560000 915 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:16:23.560000 915 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:16:23.560000 915 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3784\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.2984\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0698\n",
      "Total Training Throughput: 11762.432\n",
      "End To End Training Time: 4.081\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 8 --attention_heads 8 --schedule Interleaved1F1B --processes 2 --num_chunks 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:16:35.617002Z",
     "iopub.status.busy": "2025-10-21T02:16:35.616707Z",
     "iopub.status.idle": "2025-10-21T02:16:49.335959Z",
     "shell.execute_reply": "2025-10-21T02:16:49.335178Z",
     "shell.execute_reply.started": "2025-10-21T02:16:35.616971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:16:37.292000 953 torch/distributed/run.py:792] \n",
      "W1021 02:16:37.292000 953 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:16:37.292000 953 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:16:37.292000 953 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3700\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3097\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0819\n",
      "Total Training Throughput: 11063.757\n",
      "End To End Training Time: 4.338\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 8 --attention_heads 12 --schedule Interleaved1F1B --processes 2 --num_chunks 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:16:49.337167Z",
     "iopub.status.busy": "2025-10-21T02:16:49.336913Z",
     "iopub.status.idle": "2025-10-21T02:17:05.460948Z",
     "shell.execute_reply": "2025-10-21T02:17:05.460175Z",
     "shell.execute_reply.started": "2025-10-21T02:16:49.337144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:16:51.023000 991 torch/distributed/run.py:792] \n",
      "W1021 02:16:51.023000 991 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:16:51.023000 991 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:16:51.023000 991 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3700\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.2848\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0668\n",
      "Total Training Throughput: 8868.424\n",
      "End To End Training Time: 5.412\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 12 --attention_heads 4 --schedule Interleaved1F1B --processes 2 --num_chunks 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:17:05.462130Z",
     "iopub.status.busy": "2025-10-21T02:17:05.461925Z",
     "iopub.status.idle": "2025-10-21T02:17:22.085097Z",
     "shell.execute_reply": "2025-10-21T02:17:22.084370Z",
     "shell.execute_reply.started": "2025-10-21T02:17:05.462110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:17:07.136000 1029 torch/distributed/run.py:792] \n",
      "W1021 02:17:07.136000 1029 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:17:07.136000 1029 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:17:07.136000 1029 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3795\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3237\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0831\n",
      "Total Training Throughput: 8705.337\n",
      "End To End Training Time: 5.514\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 12 --attention_heads 8 --schedule Interleaved1F1B --processes 2 --num_chunks 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:17:22.086290Z",
     "iopub.status.busy": "2025-10-21T02:17:22.086058Z",
     "iopub.status.idle": "2025-10-21T02:17:38.860960Z",
     "shell.execute_reply": "2025-10-21T02:17:38.860169Z",
     "shell.execute_reply.started": "2025-10-21T02:17:22.086267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1021 02:17:23.794000 1067 torch/distributed/run.py:792] \n",
      "W1021 02:17:23.794000 1067 torch/distributed/run.py:792] *****************************************\n",
      "W1021 02:17:23.794000 1067 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1021 02:17:23.794000 1067 torch/distributed/run.py:792] *****************************************\n",
      "Rank: 0, Starting Epoch: 1\n",
      "Rank: 1, Starting Epoch: 1\n",
      "Rank: 1, Avg Loss: 9.3761\n",
      "Rank: 1, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 2\n",
      "Rank: 0, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.3210\n",
      "Rank: 1, Starting Epoch: 3\n",
      "Rank: 1, Avg Loss: 9.0901\n",
      "Total Training Throughput: 8293.773\n",
      "End To End Training Time: 5.787\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 ../input/temp65/distributed_training.py --layers 12 --attention_heads 12 --schedule Interleaved1F1B --processes 2 --num_chunks 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:41:52.822086Z",
     "iopub.status.busy": "2025-10-21T02:41:52.821470Z",
     "iopub.status.idle": "2025-10-21T02:41:53.167176Z",
     "shell.execute_reply": "2025-10-21T02:41:53.166531Z",
     "shell.execute_reply.started": "2025-10-21T02:41:52.822063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>n_attention_heads</th>\n",
       "      <th>n_processes</th>\n",
       "      <th>schedule</th>\n",
       "      <th>training_throughput</th>\n",
       "      <th>total_training_time</th>\n",
       "      <th>training_throughput_speedup</th>\n",
       "      <th>scaling_efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>GPipe</td>\n",
       "      <td>15281.787</td>\n",
       "      <td>3.141</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>GPipe</td>\n",
       "      <td>17518.957</td>\n",
       "      <td>2.740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>GPipe</td>\n",
       "      <td>17651.923</td>\n",
       "      <td>2.719</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>GPipe</td>\n",
       "      <td>12171.319</td>\n",
       "      <td>3.944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>GPipe</td>\n",
       "      <td>11738.217</td>\n",
       "      <td>4.089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>GPipe</td>\n",
       "      <td>11286.389</td>\n",
       "      <td>4.253</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>GPipe</td>\n",
       "      <td>9089.485</td>\n",
       "      <td>5.281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>GPipe</td>\n",
       "      <td>8906.597</td>\n",
       "      <td>5.389</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>GPipe</td>\n",
       "      <td>8216.436</td>\n",
       "      <td>5.842</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1F1B</td>\n",
       "      <td>17685.948</td>\n",
       "      <td>2.714</td>\n",
       "      <td>1.157322</td>\n",
       "      <td>57.866099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1F1B</td>\n",
       "      <td>17387.224</td>\n",
       "      <td>2.761</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>49.624027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1F1B</td>\n",
       "      <td>16730.582</td>\n",
       "      <td>2.869</td>\n",
       "      <td>0.947805</td>\n",
       "      <td>47.390253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1F1B</td>\n",
       "      <td>11778.643</td>\n",
       "      <td>4.075</td>\n",
       "      <td>0.967738</td>\n",
       "      <td>48.386880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1F1B</td>\n",
       "      <td>11584.665</td>\n",
       "      <td>4.143</td>\n",
       "      <td>0.986919</td>\n",
       "      <td>49.345931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1F1B</td>\n",
       "      <td>10936.940</td>\n",
       "      <td>4.389</td>\n",
       "      <td>0.969038</td>\n",
       "      <td>48.451901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1F1B</td>\n",
       "      <td>8666.590</td>\n",
       "      <td>5.539</td>\n",
       "      <td>0.953474</td>\n",
       "      <td>47.673713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1F1B</td>\n",
       "      <td>8664.951</td>\n",
       "      <td>5.540</td>\n",
       "      <td>0.972869</td>\n",
       "      <td>48.643444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1F1B</td>\n",
       "      <td>8056.898</td>\n",
       "      <td>5.958</td>\n",
       "      <td>0.980583</td>\n",
       "      <td>49.029153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Interleaved1F1B</td>\n",
       "      <td>17179.105</td>\n",
       "      <td>2.794</td>\n",
       "      <td>1.124156</td>\n",
       "      <td>56.207775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>Interleaved1F1B</td>\n",
       "      <td>17085.271</td>\n",
       "      <td>2.809</td>\n",
       "      <td>0.975245</td>\n",
       "      <td>48.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>Interleaved1F1B</td>\n",
       "      <td>16215.554</td>\n",
       "      <td>2.960</td>\n",
       "      <td>0.918628</td>\n",
       "      <td>45.931409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Interleaved1F1B</td>\n",
       "      <td>11887.872</td>\n",
       "      <td>4.038</td>\n",
       "      <td>0.976712</td>\n",
       "      <td>48.835595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>Interleaved1F1B</td>\n",
       "      <td>11762.432</td>\n",
       "      <td>4.081</td>\n",
       "      <td>1.002063</td>\n",
       "      <td>50.103146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>Interleaved1F1B</td>\n",
       "      <td>11063.757</td>\n",
       "      <td>4.338</td>\n",
       "      <td>0.980274</td>\n",
       "      <td>49.013715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Interleaved1F1B</td>\n",
       "      <td>8868.424</td>\n",
       "      <td>5.412</td>\n",
       "      <td>0.975679</td>\n",
       "      <td>48.783974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>Interleaved1F1B</td>\n",
       "      <td>8705.337</td>\n",
       "      <td>5.514</td>\n",
       "      <td>0.977403</td>\n",
       "      <td>48.870163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>Interleaved1F1B</td>\n",
       "      <td>8293.773</td>\n",
       "      <td>5.787</td>\n",
       "      <td>1.009412</td>\n",
       "      <td>50.470624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  n_attention_heads  n_processes         schedule  \\\n",
       "0          4                  4            2            GPipe   \n",
       "1          4                  8            2            GPipe   \n",
       "2          4                 12            2            GPipe   \n",
       "3          8                  4            2            GPipe   \n",
       "4          8                  8            2            GPipe   \n",
       "5          8                 12            2            GPipe   \n",
       "6         12                  4            2            GPipe   \n",
       "7         12                  8            2            GPipe   \n",
       "8         12                 12            2            GPipe   \n",
       "9          4                  4            2             1F1B   \n",
       "10         4                  8            2             1F1B   \n",
       "11         4                 12            2             1F1B   \n",
       "12         8                  4            2             1F1B   \n",
       "13         8                  8            2             1F1B   \n",
       "14         8                 12            2             1F1B   \n",
       "15        12                  4            2             1F1B   \n",
       "16        12                  8            2             1F1B   \n",
       "17        12                 12            2             1F1B   \n",
       "18         4                  4            2  Interleaved1F1B   \n",
       "19         4                  8            2  Interleaved1F1B   \n",
       "20         4                 12            2  Interleaved1F1B   \n",
       "21         8                  4            2  Interleaved1F1B   \n",
       "22         8                  8            2  Interleaved1F1B   \n",
       "23         8                 12            2  Interleaved1F1B   \n",
       "24        12                  4            2  Interleaved1F1B   \n",
       "25        12                  8            2  Interleaved1F1B   \n",
       "26        12                 12            2  Interleaved1F1B   \n",
       "\n",
       "    training_throughput  total_training_time  training_throughput_speedup  \\\n",
       "0             15281.787                3.141                     1.000000   \n",
       "1             17518.957                2.740                     1.000000   \n",
       "2             17651.923                2.719                     1.000000   \n",
       "3             12171.319                3.944                     1.000000   \n",
       "4             11738.217                4.089                     1.000000   \n",
       "5             11286.389                4.253                     1.000000   \n",
       "6              9089.485                5.281                     1.000000   \n",
       "7              8906.597                5.389                     1.000000   \n",
       "8              8216.436                5.842                     1.000000   \n",
       "9             17685.948                2.714                     1.157322   \n",
       "10            17387.224                2.761                     0.992481   \n",
       "11            16730.582                2.869                     0.947805   \n",
       "12            11778.643                4.075                     0.967738   \n",
       "13            11584.665                4.143                     0.986919   \n",
       "14            10936.940                4.389                     0.969038   \n",
       "15             8666.590                5.539                     0.953474   \n",
       "16             8664.951                5.540                     0.972869   \n",
       "17             8056.898                5.958                     0.980583   \n",
       "18            17179.105                2.794                     1.124156   \n",
       "19            17085.271                2.809                     0.975245   \n",
       "20            16215.554                2.960                     0.918628   \n",
       "21            11887.872                4.038                     0.976712   \n",
       "22            11762.432                4.081                     1.002063   \n",
       "23            11063.757                4.338                     0.980274   \n",
       "24             8868.424                5.412                     0.975679   \n",
       "25             8705.337                5.514                     0.977403   \n",
       "26             8293.773                5.787                     1.009412   \n",
       "\n",
       "    scaling_efficiency  \n",
       "0            50.000000  \n",
       "1            50.000000  \n",
       "2            50.000000  \n",
       "3            50.000000  \n",
       "4            50.000000  \n",
       "5            50.000000  \n",
       "6            50.000000  \n",
       "7            50.000000  \n",
       "8            50.000000  \n",
       "9            57.866099  \n",
       "10           49.624027  \n",
       "11           47.390253  \n",
       "12           48.386880  \n",
       "13           49.345931  \n",
       "14           48.451901  \n",
       "15           47.673713  \n",
       "16           48.643444  \n",
       "17           49.029153  \n",
       "18           56.207775  \n",
       "19           48.762238  \n",
       "20           45.931409  \n",
       "21           48.835595  \n",
       "22           50.103146  \n",
       "23           49.013715  \n",
       "24           48.783974  \n",
       "25           48.870163  \n",
       "26           50.470624  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experimental Evaluation Part 1\n",
    "# Experimental Evaluation Part 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "column_names = ['n_layers', 'n_attention_heads', 'n_processes', 'schedule', 'training_throughput', 'total_training_time', 'training_throughput_speedup', 'scaling_efficiency']\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "df.loc[len(df)] = [4, 4, 2, \"GPipe\", 15281.787, 3.141, 1.0, (1.0/2) * 100]\n",
    "df.loc[len(df)] = [4, 8, 2, \"GPipe\", 17518.957, 2.740, 1.0, (1.0/2) * 100]\n",
    "df.loc[len(df)] = [4, 12, 2, \"GPipe\", 17651.923, 2.719, 1.0, (1.0/2) * 100]\n",
    "df.loc[len(df)] = [8, 4, 2, \"GPipe\", 12171.319, 3.944, 1.0, (1.0/2) * 100]\n",
    "df.loc[len(df)] = [8, 8, 2, \"GPipe\", 11738.217, 4.089, 1.0, (1.0/2) * 100]\n",
    "df.loc[len(df)] = [8, 12, 2, \"GPipe\", 11286.389, 4.253, 1.0, (1.0/2) * 100]\n",
    "df.loc[len(df)] = [12, 4, 2, \"GPipe\", 9089.485, 5.281, 1.0, (1.0/2) * 100]\n",
    "df.loc[len(df)] = [12, 8, 2, \"GPipe\", 8906.597, 5.389, 1.0, (1.0/2) * 100]\n",
    "df.loc[len(df)] = [12, 12, 2, \"GPipe\", 8216.436, 5.842, 1.0, (1.0/2) * 100]\n",
    "\n",
    "df.loc[len(df)] = [4, 4, 2, \"1F1B\", 17685.948, 2.714, 17685.948 / 15281.787, ((17685.948 / 15281.787) / 2) * 100]\n",
    "df.loc[len(df)] = [4, 8, 2, \"1F1B\", 17387.224, 2.761, 17387.224 / 17518.957, ((17387.224 / 17518.957) / 2) * 100]\n",
    "df.loc[len(df)] = [4, 12, 2, \"1F1B\", 16730.582, 2.869, 16730.582 / 17651.923, ((16730.582 / 17651.923) / 2) * 100]\n",
    "df.loc[len(df)] = [8, 4, 2, \"1F1B\", 11778.643, 4.075, 11778.643 / 12171.319, ((11778.643 / 12171.319) / 2) * 100]\n",
    "df.loc[len(df)] = [8, 8, 2, \"1F1B\", 11584.665, 4.143, 11584.665 / 11738.217, ((11584.665 / 11738.217) / 2) * 100]\n",
    "df.loc[len(df)] = [8, 12, 2, \"1F1B\", 10936.940, 4.389, 10936.940 / 11286.389, ((10936.940 / 11286.389) / 2) * 100]\n",
    "df.loc[len(df)] = [12, 4, 2, \"1F1B\", 8666.590, 5.539, 8666.590 / 9089.485, ((8666.590 / 9089.485) / 2) * 100]\n",
    "df.loc[len(df)] = [12, 8, 2, \"1F1B\", 8664.951, 5.540, 8664.951 / 8906.597, ((8664.951 / 8906.597) / 2) * 100]\n",
    "df.loc[len(df)] = [12, 12, 2, \"1F1B\", 8056.898, 5.958, 8056.898 / 8216.436, ((8056.898 / 8216.436) / 2) * 100]\n",
    "\n",
    "df.loc[len(df)] = [4, 4, 2, \"Interleaved1F1B\", 17179.105, 2.794, 17179.105 / 15281.787, ((17179.105 / 15281.787) / 2) * 100]\n",
    "df.loc[len(df)] = [4, 8, 2, \"Interleaved1F1B\", 17085.271, 2.809, 17085.271 / 17518.957, ((17085.271 / 17518.957) / 2) * 100]\n",
    "df.loc[len(df)] = [4, 12, 2, \"Interleaved1F1B\", 16215.554, 2.960, 16215.554 / 17651.923, ((16215.554 / 17651.923) / 2) * 100]\n",
    "df.loc[len(df)] = [8, 4, 2, \"Interleaved1F1B\", 11887.872, 4.038, 11887.872 / 12171.319, ((11887.872 / 12171.319) / 2) * 100]\n",
    "df.loc[len(df)] = [8, 8, 2, \"Interleaved1F1B\", 11762.432, 4.081, 11762.432 / 11738.217, ((11762.432 / 11738.217) / 2) * 100]\n",
    "df.loc[len(df)] = [8, 12, 2, \"Interleaved1F1B\", 11063.757, 4.338, 11063.757 / 11286.389, ((11063.757 / 11286.389) / 2) * 100]\n",
    "df.loc[len(df)] = [12, 4, 2, \"Interleaved1F1B\", 8868.424, 5.412, 8868.424 / 9089.485, ((8868.424 / 9089.485) / 2) * 100]\n",
    "df.loc[len(df)] = [12, 8, 2, \"Interleaved1F1B\", 8705.337, 5.514, 8705.337 / 8906.597, ((8705.337 / 8906.597) / 2) * 100]\n",
    "df.loc[len(df)] = [12, 12, 2, \"Interleaved1F1B\", 8293.773, 5.787, 8293.773 / 8216.436, ((8293.773 / 8216.436) / 2) * 100]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:42:23.027280Z",
     "iopub.status.busy": "2025-10-21T02:42:23.026440Z",
     "iopub.status.idle": "2025-10-21T02:42:23.046991Z",
     "shell.execute_reply": "2025-10-21T02:42:23.046209Z",
     "shell.execute_reply.started": "2025-10-21T02:42:23.027258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    n_layers  n_attention_heads  n_processes         schedule  \\\n",
      "0          4                  4            2            GPipe   \n",
      "1          4                  8            2            GPipe   \n",
      "2          4                 12            2            GPipe   \n",
      "3          8                  4            2            GPipe   \n",
      "4          8                  8            2            GPipe   \n",
      "5          8                 12            2            GPipe   \n",
      "6         12                  4            2            GPipe   \n",
      "7         12                  8            2            GPipe   \n",
      "8         12                 12            2            GPipe   \n",
      "9          4                  4            2             1F1B   \n",
      "10         4                  8            2             1F1B   \n",
      "11         4                 12            2             1F1B   \n",
      "12         8                  4            2             1F1B   \n",
      "13         8                  8            2             1F1B   \n",
      "14         8                 12            2             1F1B   \n",
      "15        12                  4            2             1F1B   \n",
      "16        12                  8            2             1F1B   \n",
      "17        12                 12            2             1F1B   \n",
      "18         4                  4            2  Interleaved1F1B   \n",
      "19         4                  8            2  Interleaved1F1B   \n",
      "20         4                 12            2  Interleaved1F1B   \n",
      "21         8                  4            2  Interleaved1F1B   \n",
      "22         8                  8            2  Interleaved1F1B   \n",
      "23         8                 12            2  Interleaved1F1B   \n",
      "24        12                  4            2  Interleaved1F1B   \n",
      "25        12                  8            2  Interleaved1F1B   \n",
      "26        12                 12            2  Interleaved1F1B   \n",
      "\n",
      "    training_throughput_speedup  \n",
      "0                      1.000000  \n",
      "1                      1.000000  \n",
      "2                      1.000000  \n",
      "3                      1.000000  \n",
      "4                      1.000000  \n",
      "5                      1.000000  \n",
      "6                      1.000000  \n",
      "7                      1.000000  \n",
      "8                      1.000000  \n",
      "9                      1.157322  \n",
      "10                     0.992481  \n",
      "11                     0.947805  \n",
      "12                     0.967738  \n",
      "13                     0.986919  \n",
      "14                     0.969038  \n",
      "15                     0.953474  \n",
      "16                     0.972869  \n",
      "17                     0.980583  \n",
      "18                     1.124156  \n",
      "19                     0.975245  \n",
      "20                     0.918628  \n",
      "21                     0.976712  \n",
      "22                     1.002063  \n",
      "23                     0.980274  \n",
      "24                     0.975679  \n",
      "25                     0.977403  \n",
      "26                     1.009412  \n",
      "\n",
      "    n_layers  n_attention_heads  n_processes         schedule  \\\n",
      "0          4                  4            2            GPipe   \n",
      "1          4                  8            2            GPipe   \n",
      "2          4                 12            2            GPipe   \n",
      "3          8                  4            2            GPipe   \n",
      "4          8                  8            2            GPipe   \n",
      "5          8                 12            2            GPipe   \n",
      "6         12                  4            2            GPipe   \n",
      "7         12                  8            2            GPipe   \n",
      "8         12                 12            2            GPipe   \n",
      "9          4                  4            2             1F1B   \n",
      "10         4                  8            2             1F1B   \n",
      "11         4                 12            2             1F1B   \n",
      "12         8                  4            2             1F1B   \n",
      "13         8                  8            2             1F1B   \n",
      "14         8                 12            2             1F1B   \n",
      "15        12                  4            2             1F1B   \n",
      "16        12                  8            2             1F1B   \n",
      "17        12                 12            2             1F1B   \n",
      "18         4                  4            2  Interleaved1F1B   \n",
      "19         4                  8            2  Interleaved1F1B   \n",
      "20         4                 12            2  Interleaved1F1B   \n",
      "21         8                  4            2  Interleaved1F1B   \n",
      "22         8                  8            2  Interleaved1F1B   \n",
      "23         8                 12            2  Interleaved1F1B   \n",
      "24        12                  4            2  Interleaved1F1B   \n",
      "25        12                  8            2  Interleaved1F1B   \n",
      "26        12                 12            2  Interleaved1F1B   \n",
      "\n",
      "    scaling_efficiency  \n",
      "0            50.000000  \n",
      "1            50.000000  \n",
      "2            50.000000  \n",
      "3            50.000000  \n",
      "4            50.000000  \n",
      "5            50.000000  \n",
      "6            50.000000  \n",
      "7            50.000000  \n",
      "8            50.000000  \n",
      "9            57.866099  \n",
      "10           49.624027  \n",
      "11           47.390253  \n",
      "12           48.386880  \n",
      "13           49.345931  \n",
      "14           48.451901  \n",
      "15           47.673713  \n",
      "16           48.643444  \n",
      "17           49.029153  \n",
      "18           56.207775  \n",
      "19           48.762238  \n",
      "20           45.931409  \n",
      "21           48.835595  \n",
      "22           50.103146  \n",
      "23           49.013715  \n",
      "24           48.783974  \n",
      "25           48.870163  \n",
      "26           50.470624  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experimental Evaluation Part 2\n",
    "\n",
    "# Speedup Plot\n",
    "selected_columns = df[['n_layers', 'n_attention_heads', 'n_processes', 'schedule', 'training_throughput_speedup']]\n",
    "print(selected_columns)\n",
    "print()\n",
    "\n",
    "# Scaling Efficiency Plot\n",
    "selected_columns = df[['n_layers', 'n_attention_heads', 'n_processes', 'schedule', 'scaling_efficiency']]\n",
    "print(selected_columns)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T03:19:52.163023Z",
     "iopub.status.busy": "2025-10-21T03:19:52.162732Z",
     "iopub.status.idle": "2025-10-21T03:19:52.168539Z",
     "shell.execute_reply": "2025-10-21T03:19:52.167917Z",
     "shell.execute_reply.started": "2025-10-21T03:19:52.163002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAs seen in the results produced above, GPipe performed better than the 1F1B and Interleaved 1F1B schedules for the most part. The only configuration where 1F1B actually performed better than GPipe was for the 4-layer, 4-attention-head, 2-process configuration. The only configurations where Interleaved 1F1B actually performed better than GPipe were for the 4-layer, 4-attention-head, 2-process configuration, the 8-layer, 8-attention-head, 2-process configuration, and the 12-layer, 12-attention-head, 2-process configuration. The absence of a visible or quantifiable improvement for the 1F1B and Interleaved 1F1B schedules in comparison to the GPipe schedule was probably because of how small the provided data was. Since the input dataset(1 batch) was pretty small, there was probably not as much need for parallelism so the extra communication overhead from the 1F1B and Interleaved 1F1B schedules may have proven to be more damaging to the overall performance. The performance relative to GPipe scheduling kind of displayed more consistently higher values whenever model size was increased to 12 layers for the 1F1B and Interleaved1F1B schedules. For 8 layers and 4 layers, the performance relative to GPipe scheduling kind of oscillated more between relatively higher and lower throughputs for both the 1F1B and Interleaved1F1B schedules. In addition, across all model sizes, when the total number of layers was kept constant, adding more attention heads tended to typically improve the performance of the Interleaved1F1B and 1F1B schedules relative to GPipe scheduling for the larger model sizes(12 layers). No noticeable improvement or decrease in performance really occurred for the smaller model sizes(8 layers and 4 layers) as more attention heads were added.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experimental Evaluation Part 3\n",
    "\n",
    "\"\"\"\n",
    "As seen in the results produced above, GPipe performed better than the 1F1B and Interleaved 1F1B schedules for the most part. The only configuration where 1F1B actually performed better than GPipe was for the 4-layer, 4-attention-head, 2-process configuration. The only configurations where Interleaved 1F1B actually performed better than GPipe were for the 4-layer, 4-attention-head, 2-process configuration, the 8-layer, 8-attention-head, 2-process configuration, and the 12-layer, 12-attention-head, 2-process configuration. The absence of a visible or quantifiable improvement for the 1F1B and Interleaved 1F1B schedules in comparison to the GPipe schedule was probably because of how small the provided data was. Since the input dataset(1 batch) was pretty small, there was probably not as much need for parallelism so the extra communication overhead from the 1F1B and Interleaved 1F1B schedules may have proven to be more damaging to the overall performance. The performance relative to GPipe scheduling kind of displayed more consistently higher values whenever model size was increased to 12 layers for the 1F1B and Interleaved1F1B schedules. For 8 layers and 4 layers, the performance relative to GPipe scheduling kind of oscillated more between relatively higher and lower throughputs for both the 1F1B and Interleaved1F1B schedules. In addition, across all model sizes, when the total number of layers was kept constant, adding more attention heads tended to typically improve the performance of the Interleaved1F1B and 1F1B schedules relative to GPipe scheduling for the larger model sizes(12 layers). No noticeable improvement or decrease in performance really occurred for the smaller model sizes(8 layers and 4 layers) as more attention heads were added.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8536400,
     "sourceId": 13448408,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
