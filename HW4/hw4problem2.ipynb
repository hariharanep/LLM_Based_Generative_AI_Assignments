{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### LLM Inference Benchmarking Assignment (25 points)\n\n#### Overview\nIn this assignment, you will learn how to benchmark Large Language Model (LLM) inference using vLLM, a high-performance inference engine. You will use a small model (OPT-125M) to understand the basics of throughput measurement and the impact of different parameters on inference speed.","metadata":{"id":"YJegWPonp4Nm"}},{"cell_type":"markdown","source":"#### Environment Setup\n\n- Successfully install all required packages (2 points)\n\n\nFirst, install the required packages:","metadata":{"id":"73ge34nwqjva"}},{"cell_type":"code","source":"!pip install \"numpy<2\"\n!pip install transformers torch pandas matplotlib seaborn vllm","metadata":{"id":"_yA9ENugp0TX","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:51:44.201737Z","iopub.execute_input":"2025-11-18T06:51:44.202064Z","iopub.status.idle":"2025-11-18T06:51:51.707244Z","shell.execute_reply.started":"2025-11-18T06:51:44.202046Z","shell.execute_reply":"2025-11-18T06:51:51.706221Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.57.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.7)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\nRequirement already satisfied: vllm in /usr/local/lib/python3.11/dist-packages (0.11.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1.3)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (75.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (6.2.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (7.1.3)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\nRequirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.8)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (6.33.0)\nRequirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.116.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.13.2)\nRequirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.7.1)\nRequirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.12.4)\nRequirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.22.1)\nRequirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (7.1.0)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\nRequirement already satisfied: lm-format-enforcer==0.11.3 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.11.3)\nRequirement already satisfied: llguidance<0.8.0,>=0.7.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.7.30)\nRequirement already satisfied: outlines_core==0.2.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.11)\nRequirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.11/dist-packages (from vllm) (5.6.3)\nRequirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.2.2)\nRequirement already satisfied: xgrammar==0.1.25 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.25)\nRequirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.1.1.post7)\nRequirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (26.2.1)\nRequirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm) (0.19.0)\nRequirement already satisfied: gguf>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.17.1)\nRequirement already satisfied: mistral_common>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from mistral_common[audio,image]>=1.8.2->vllm) (1.8.5)\nRequirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\nRequirement already satisfied: compressed-tensors==0.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.11.0)\nRequirement already satisfied: depyf==0.19.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.19.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.2)\nRequirement already satisfied: watchfiles in /usr/local/lib/python3.11/dist-packages (from vllm) (1.1.1)\nRequirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (4.0.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.3)\nRequirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.13.0)\nRequirement already satisfied: pybase64 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.4.2)\nRequirement already satisfied: cbor2 in /usr/local/lib/python3.11/dist-packages (from vllm) (5.7.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from vllm) (1.3.7)\nRequirement already satisfied: openai-harmony>=0.0.3 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.0.8)\nRequirement already satisfied: numba==0.61.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.61.2)\nRequirement already satisfied: ray>=2.48.0 in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]>=2.48.0->vllm) (2.51.1)\nRequirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.8.0)\nRequirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.23.0)\nRequirement already satisfied: xformers==0.0.32.post1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.0.32.post1)\nRequirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from compressed-tensors==0.11.0->vllm) (2.4.6)\nRequirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.19.0->vllm) (0.8.1)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.19.0->vllm) (0.4.0)\nRequirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer==0.11.3->vllm) (0.3.3)\nRequirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.61.2->vllm) (0.44.0)\nRequirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.47.2)\nRequirement already satisfied: fastapi-cli>=0.0.8 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.16)\nRequirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\nRequirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\nRequirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.35.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (4.25.0)\nRequirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.11/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.10.6)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.99.1->vllm) (4.11.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.99.1->vllm) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.99.1->vllm) (0.10.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.99.1->vllm) (1.3.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.7->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.7->vllm) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.7->vllm) (0.4.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: click!=8.3.0,>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (8.3.1)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.2)\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]>=2.48.0->vllm) (13.6.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.22.0)\nRequirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\nRequirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.16.0)\nRequirement already satisfied: rich-toolkit>=0.14.8 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.1)\nRequirement already satisfied: fastapi-cloud-cli>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.3.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.26.0)\nRequirement already satisfied: pycountry>=23 in /usr/local/lib/python3.11/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (24.6.1)\nRequirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.1)\nRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.2.1)\nRequirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.22.1)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.13.1)\nRequirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.5.0.post1)\nRequirement already satisfied: rignore>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.6)\nRequirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.33.2)\nRequirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.2.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.0.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.23)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Import the necessary libraries:","metadata":{"id":"njDhTvc1ubaf"}},{"cell_type":"code","source":"# Import necessary libraries\nimport dataclasses\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\nimport gc\nimport torch\n\nprint(\"All libraries imported successfully!\")","metadata":{"id":"R6ZJMaIIuYg9","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:51:59.122554Z","iopub.execute_input":"2025-11-18T06:51:59.122857Z","iopub.status.idle":"2025-11-18T06:52:10.828222Z","shell.execute_reply.started":"2025-11-18T06:51:59.122825Z","shell.execute_reply":"2025-11-18T06:52:10.827571Z"}},"outputs":[{"name":"stdout","text":"INFO 11-18 06:52:03 [__init__.py:216] Automatically detected platform cuda.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-18 06:52:04.539482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763448724.563899    6422 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763448724.573948    6422 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"All libraries imported successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"#### Part 1: Creating Sample Requests\n\n- Implement the create_synthetic_requests function correctly (5 points)\n\nWe'll create a function to generate sample requests for benchmarking:","metadata":{"id":"y5AIcnJPugAu"}},{"cell_type":"code","source":"import random\n\n@dataclasses.dataclass\nclass SampleRequest:\n    \"\"\"A class representing a single inference request for benchmarking.\"\"\"\n    prompt: str\n    prompt_len: int\n    expected_output_len: int\n\ndef create_synthetic_requests(\n    tokenizer,\n    num_requests: int,\n    input_len: int,\n    output_len: int\n) -> List[SampleRequest]:\n    \"\"\"Create synthetic requests for benchmarking.\n\n    Args:\n        tokenizer: The tokenizer to use\n        num_requests: Number of requests to generate\n        input_len: Desired input length in tokens\n        output_len: Desired output length in tokens\n\n    Returns:\n        List of SampleRequest objects\n    \"\"\"\n    requests = []\n\n    prompts = [\n        \"Tell me about the difference between recurrent neural networks and bidirectional long short term memory networks?\",\n        \"Describe the entire history of the planet Earth?\",\n        \"What is the source of the entire worlds' problems?\",\n        \"Is it true that P Diddy killed Tupac and Biggie and what evidence is there confirming this?\",\n        \"Is Angel Reese really bad at basketball or do people just hate her for no reason?\",\n        \"How can I engineer a newborn child to be exactly like Lebron James?\",\n        \"What's the best fried chicken spot in the entire USA?\",\n        \"Why did the Nation of Islam team up with the CIA to murder Malcolm X?\",\n        \"Did Sony pay a doctor to prescribe drugs to kill Michael Jackson on purpose?\",\n        \"Based on all the existing evidence out there, do you think Deshaun Watson should be in jail?\"\n    ]\n\n    prompt_ending = \" Please answer this question thoroughly and provide detailed explanation and analysis to back up your answer.\"\n\n    filler = [\"um\", \"uh\", \"er\", \"ah\", \"like\", \"you know\", \"well\", \"so\", \"I mean\", \"basically\", \"actually\", \"literally\", \"seriously\", \"totally\", \"right\", \"okay\", \"mhm\", \"uh huh\", \"anyway\", \"just\", \"kind of\", \"sort of\", \"I guess\", \"I suppose\", \"you see\", \"at the end of the day\", \"believe me\", \"for what it's worth\", \"needless to say\", \"hmm\"]\n\n    for i in range(num_requests):\n        prompt = prompts[i % len(prompts)] + prompt_ending\n        \n        while len(tokenizer.encode(prompt)) < input_len:\n            prompt += \" \"\n            prompt += random.choice(filler)\n\n        tokens = tokenizer.encode(prompt)[:input_len]\n        prompt = tokenizer.decode(tokens)\n            \n    \n        requests.append(SampleRequest(\n            prompt=prompt,\n            prompt_len=len(tokens),\n            expected_output_len=output_len\n        ))\n\n    return requests","metadata":{"id":"AIDbFwazvI8E","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:52:14.058423Z","iopub.execute_input":"2025-11-18T06:52:14.059421Z","iopub.status.idle":"2025-11-18T06:52:14.066462Z","shell.execute_reply.started":"2025-11-18T06:52:14.059392Z","shell.execute_reply":"2025-11-18T06:52:14.065896Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"#### Part 2: Implementing the Benchmark\n- Implement the run_benchmark function correctly (8 points)\n\nCreate the main benchmarking function:","metadata":{"id":"N8T9s0havPf9"}},{"cell_type":"code","source":"def run_benchmark(\n    requests: List[SampleRequest],\n    model_name: str,\n    tensor_parallel_size: int = 1,\n    gpu_memory_utilization: float = 0.9,\n    max_num_batched_tokens: int = 2048,\n    n: int = 1\n) -> float:\n    \"\"\"Run inference benchmark using vLLM.\n\n    Args:\n        requests: List of requests to process\n        model_name: Name of the model to benchmark\n        tensor_parallel_size: Number of GPUs for tensor parallelism\n        gpu_memory_utilization: Target GPU memory utilization\n        max_num_batched_tokens: Maximum number of tokens in a batch\n        n: Number of sequences to generate per prompt\n\n    Returns:\n        Elapsed time in seconds\n    \"\"\"\n    \n    llm = LLM(\n        model=model_name,\n        tensor_parallel_size=tensor_parallel_size,\n        gpu_memory_utilization=gpu_memory_utilization,\n        max_num_batched_tokens=max_num_batched_tokens\n    )\n\n    prompts = [req.prompt for req in requests]\n    \n    sampling_params = SamplingParams(\n        temperature=0.0,\n        max_tokens=requests[0].expected_output_len,\n        n=n,\n        ignore_eos=True \n    )\n\n    llm.generate(prompts[0], sampling_params)\n\n    start_time = time.time()\n    outputs = llm.generate(prompts, sampling_params)\n    elapsed_time = time.time() - start_time\n\n    del llm\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return elapsed_time\n    ","metadata":{"id":"1iVZVVEDvNFf","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:52:16.885434Z","iopub.execute_input":"2025-11-18T06:52:16.886281Z","iopub.status.idle":"2025-11-18T06:52:16.892080Z","shell.execute_reply.started":"2025-11-18T06:52:16.886249Z","shell.execute_reply":"2025-11-18T06:52:16.891246Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"#### Part 3: Running the Benchmark (15 points)\n\n- Experiments and Analysis (10 points)\n\n  - Run the benchmark with different batch sizes (2 points)","metadata":{"id":"Sg0291u7vZd6"}},{"cell_type":"markdown","source":"  - Run the benchmark with different input/output lengths (2 points)\n","metadata":{"id":"Ab3WdbEhxurz"}},{"cell_type":"code","source":"def experiment_batch_sizes(model_name: str, batch_sizes: List[int]) -> pd.DataFrame:\n    \"\"\"Run benchmark with different batch sizes.\n\n    Args:\n        model_name: Name of the model to benchmark\n        batch_sizes: List of batch sizes to test\n\n    Returns:\n        DataFrame with results\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    results = []\n    \n    for batch_size in batch_sizes:\n        requests = create_synthetic_requests(tokenizer, 50, 128, 128)\n        elapsed = run_benchmark(requests, model_name, max_num_batched_tokens=batch_size)\n        \n        total_tokens = sum(r.prompt_len + r.expected_output_len for r in requests)\n        throughput = total_tokens / elapsed\n        \n        results.append({\n            'batch_size': batch_size,\n            'throughput': throughput,\n            'elapsed_time': elapsed\n        })\n    \n    return pd.DataFrame(results)","metadata":{"id":"OegkDTQaxn0f","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:52:20.051049Z","iopub.execute_input":"2025-11-18T06:52:20.051384Z","iopub.status.idle":"2025-11-18T06:52:20.056419Z","shell.execute_reply.started":"2025-11-18T06:52:20.051360Z","shell.execute_reply":"2025-11-18T06:52:20.055769Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def experiment_sequence_lengths(model_name: str, lengths: List[int]) -> pd.DataFrame:\n    \"\"\"Run benchmark with different input/output lengths.\n\n    Args:\n        model_name: Name of the model to benchmark\n        lengths: List of sequence lengths to test\n\n    Returns:\n        DataFrame with results\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    results = []\n    \n    for length in lengths:\n        requests = create_synthetic_requests(tokenizer, 50, length, length)\n        elapsed = run_benchmark(requests, model_name)\n        \n        total_tokens = sum(r.prompt_len + r.expected_output_len for r in requests)\n        throughput = total_tokens / elapsed\n        \n        results.append({\n            'sequence_length': length,\n            'throughput': throughput,\n            'elapsed_time': elapsed\n        })\n    \n    return pd.DataFrame(results)","metadata":{"id":"-FrT737XyACa","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:52:22.804297Z","iopub.execute_input":"2025-11-18T06:52:22.804898Z","iopub.status.idle":"2025-11-18T06:52:22.810318Z","shell.execute_reply.started":"2025-11-18T06:52:22.804868Z","shell.execute_reply":"2025-11-18T06:52:22.809490Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"  - Run the benchmark with different numbers of requests (2 points)\n","metadata":{"id":"eH2jvprkxxl8"}},{"cell_type":"code","source":"def experiment_num_requests(model_name: str, request_counts: List[int]) -> pd.DataFrame:\n    \"\"\"Run benchmark with different numbers of requests.\n\n    Args:\n        model_name: Name of the model to benchmark\n        request_counts: List of request counts to test\n\n    Returns:\n        DataFrame with results\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    results = []\n    \n    for count in request_counts:\n        requests = create_synthetic_requests(tokenizer, count, 128, 128)\n        elapsed = run_benchmark(requests, model_name)\n        \n        total_tokens = sum(r.prompt_len + r.expected_output_len for r in requests)\n        throughput = total_tokens / elapsed\n        \n        results.append({\n            'num_requests': count,\n            'throughput': throughput,\n            'elapsed_time': elapsed\n        })\n    \n    return pd.DataFrame(results)","metadata":{"id":"htBUI94yyBXd","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:52:25.313819Z","iopub.execute_input":"2025-11-18T06:52:25.314081Z","iopub.status.idle":"2025-11-18T06:52:25.319305Z","shell.execute_reply.started":"2025-11-18T06:52:25.314064Z","shell.execute_reply":"2025-11-18T06:52:25.318481Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"  - Create a graph showing the relationship between batch size and throughput (2 points)\n\nNow let's put everything together and run the benchmark:\n","metadata":{"id":"z7wimvKqx0ID"}},{"cell_type":"code","source":"def run_all_experiments(model_name: str = \"facebook/opt-125m\"):\n    \"\"\"Run all experiments and create visualizations.\"\"\"\n\n    # Experiment configurations\n    batch_sizes = [1024, 2048]\n    sequence_lengths = [32, 64, 128]\n    request_counts = [10, 50, 100, 200, 500]\n\n    # Run experiments\n    print(\"Running batch size experiments...\")\n    batch_results = experiment_batch_sizes(model_name, batch_sizes)\n\n    print(\"Running sequence length experiments...\")\n    length_results = experiment_sequence_lengths(model_name, sequence_lengths)\n\n    print(\"Running request count experiments...\")\n    request_results = experiment_num_requests(model_name, request_counts)\n\n    # Create visualizations\n    plt.figure(figsize=(15, 5))\n\n    # Batch size vs throughput\n    plt.subplot(1, 3, 1)\n    sns.lineplot(data=batch_results, x='batch_size', y='throughput')\n    plt.title('Batch Size vs Throughput')\n    plt.xlabel('Batch Size')\n    plt.ylabel('Tokens/second')\n\n    # Sequence length vs throughput\n    plt.subplot(1, 3, 2)\n    sns.lineplot(data=length_results, x='sequence_length', y='throughput')\n    plt.title('Sequence Length vs Throughput')\n    plt.xlabel('Sequence Length')\n    plt.ylabel('Tokens/second')\n\n    # Number of requests vs throughput\n    plt.subplot(1, 3, 3)\n    sns.lineplot(data=request_results, x='num_requests', y='throughput')\n    plt.title('Number of Requests vs Throughput')\n    plt.xlabel('Number of Requests')\n    plt.ylabel('Tokens/second')\n\n    plt.tight_layout()\n    plt.show()\n\n    return batch_results, length_results, request_results\n\n# Run experiments and store results\nbatch_results, length_results, request_results = run_all_experiments()","metadata":{"id":"Ch3Fp_-FvWDm","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T07:01:12.113223Z","iopub.execute_input":"2025-11-18T07:01:12.113799Z","iopub.status.idle":"2025-11-18T07:12:26.342208Z","shell.execute_reply.started":"2025-11-18T07:01:12.113772Z","shell.execute_reply":"2025-11-18T07:12:26.341575Z"}},"outputs":[{"name":"stdout","text":"Running batch size experiments...\nINFO 11-18 07:01:12 [utils.py:233] non-default args: {'max_num_batched_tokens': 1024, 'disable_log_stats': True, 'model': 'facebook/opt-125m'}\nINFO 11-18 07:01:13 [model.py:547] Resolved architecture: OPTForCausalLM\nINFO 11-18 07:01:13 [model.py:1510] Using max model len 2048\nINFO 11-18 07:01:13 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=1024.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-18 07:01:19.669352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763449279.692327    7967 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763449279.700060    7967 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:01:25 [__init__.py:216] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:27 [core.py:644] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:27 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m ERROR 11-18 07:01:28 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n","output_type":"stream"},{"name":"stderr","text":"[W1118 07:01:39.534502610 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W1118 07:01:49.545189927 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:49 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m WARNING 11-18 07:01:49 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:49 [gpu_model_runner.py:2602] Starting to load model facebook/opt-125m...\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:49 [gpu_model_runner.py:2634] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:49 [cuda.py:372] Using FlexAttention backend on V1 engine.\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:50 [weight_utils.py:392] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n","output_type":"stream"},{"name":"stderr","text":"Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.56it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.56it/s]\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:50 [default_loader.py:267] Loading weights took 0.29 seconds\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:51 [gpu_model_runner.py:2653] Model loading took 0.2405 GiB and 0.805210 seconds\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:54 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/932ffef25e/rank_0_0/backbone for vLLM's torch.compile\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:54 [backends.py:559] Dynamo bytecode transform time: 2.43 s\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:54 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.296 s\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:55 [monitor.py:34] torch.compile takes 2.43 s in total\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:56 [gpu_worker.py:298] Available KV cache memory: 12.53 GiB\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:56 [kv_cache_utils.py:1087] GPU KV cache size: 364,880 tokens\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:56 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 178.16x\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m WARNING 11-18 07:01:56 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 61.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:58 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.19 GiB\n\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m INFO 11-18 07:01:58 [core.py:210] init engine (profile, create kv cache, warmup model) took 6.91 seconds\nINFO 11-18 07:01:59 [llm.py:306] Supported_tasks: ['generate']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c904bc9c7c44bfabfe7cc6c7843921d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0a90609648949fcbb195d7f625f3d8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c90b67cb23e4653af20419327afa411"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/50 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef4c556546ad4003ac1443440c90bb09"}},"metadata":{}},{"name":"stderr","text":"[rank0]:[W1118 07:02:11.674677223 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:02:12 [utils.py:233] non-default args: {'max_num_batched_tokens': 2048, 'disable_log_stats': True, 'model': 'facebook/opt-125m'}\nINFO 11-18 07:02:13 [model.py:547] Resolved architecture: OPTForCausalLM\nINFO 11-18 07:02:13 [model.py:1510] Using max model len 2048\nINFO 11-18 07:02:13 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-18 07:02:18.697723: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763449338.722892    8056 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763449338.731182    8056 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:02:24 [__init__.py:216] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:26 [core.py:644] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:26 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m ERROR 11-18 07:02:27 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n","output_type":"stream"},{"name":"stderr","text":"[W1118 07:02:38.516344655 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W1118 07:02:48.522151424 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:48 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m WARNING 11-18 07:02:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:48 [gpu_model_runner.py:2602] Starting to load model facebook/opt-125m...\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:48 [gpu_model_runner.py:2634] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:48 [cuda.py:372] Using FlexAttention backend on V1 engine.\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:49 [weight_utils.py:392] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n","output_type":"stream"},{"name":"stderr","text":"Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:49 [default_loader.py:267] Loading weights took 0.28 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:50 [gpu_model_runner.py:2653] Model loading took 0.2393 GiB and 0.783940 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:53 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/932ffef25e/rank_0_0/backbone for vLLM's torch.compile\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:53 [backends.py:559] Dynamo bytecode transform time: 2.41 s\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:53 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.287 s\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:54 [monitor.py:34] torch.compile takes 2.41 s in total\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:55 [gpu_worker.py:298] Available KV cache memory: 12.53 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:55 [kv_cache_utils.py:1087] GPU KV cache size: 364,864 tokens\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:55 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 178.16x\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m WARNING 11-18 07:02:55 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 62.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:57 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.19 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8056)\u001b[0;0m INFO 11-18 07:02:57 [core.py:210] init engine (profile, create kv cache, warmup model) took 6.87 seconds\nINFO 11-18 07:02:58 [llm.py:306] Supported_tasks: ['generate']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d743bfefda82443eb6933c280c38ca52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f4f8b989bfb455fae689043f7ba6870"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ad1be02b844e5abae83b4c07012316"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/50 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcc55f251bd04e45934365206939cf89"}},"metadata":{}},{"name":"stderr","text":"[rank0]:[W1118 07:03:09.025366811 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"},{"name":"stdout","text":"Running sequence length experiments...\nINFO 11-18 07:03:10 [utils.py:233] non-default args: {'max_num_batched_tokens': 2048, 'disable_log_stats': True, 'model': 'facebook/opt-125m'}\nINFO 11-18 07:03:11 [model.py:547] Resolved architecture: OPTForCausalLM\nINFO 11-18 07:03:11 [model.py:1510] Using max model len 2048\nINFO 11-18 07:03:11 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-18 07:03:16.946941: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763449396.970434    8145 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763449396.978339    8145 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:03:23 [__init__.py:216] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:24 [core.py:644] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:24 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m ERROR 11-18 07:03:26 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n","output_type":"stream"},{"name":"stderr","text":"[W1118 07:03:36.748873536 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W1118 07:03:46.759671158 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:46 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m WARNING 11-18 07:03:46 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:46 [gpu_model_runner.py:2602] Starting to load model facebook/opt-125m...\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:47 [gpu_model_runner.py:2634] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:47 [cuda.py:372] Using FlexAttention backend on V1 engine.\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:47 [weight_utils.py:392] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n","output_type":"stream"},{"name":"stderr","text":"Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.67it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.67it/s]\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:48 [default_loader.py:267] Loading weights took 0.28 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:48 [gpu_model_runner.py:2653] Model loading took 0.2393 GiB and 0.872040 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:51 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/932ffef25e/rank_0_0/backbone for vLLM's torch.compile\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:51 [backends.py:559] Dynamo bytecode transform time: 2.40 s\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:52 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.295 s\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:52 [monitor.py:34] torch.compile takes 2.40 s in total\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:53 [gpu_worker.py:298] Available KV cache memory: 12.53 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:53 [kv_cache_utils.py:1087] GPU KV cache size: 364,864 tokens\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:53 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 178.16x\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m WARNING 11-18 07:03:53 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 63.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:55 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.19 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8145)\u001b[0;0m INFO 11-18 07:03:55 [core.py:210] init engine (profile, create kv cache, warmup model) took 6.84 seconds\nINFO 11-18 07:03:56 [llm.py:306] Supported_tasks: ['generate']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"556a6f04553848dead9efd4942b3bd7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d1666558df9484b947bbd04cfa4a7ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"957bd7cf5f2b4947bfa1cdf14456d335"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/50 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2daf9f45d1434a518f6f10c2b2def1c1"}},"metadata":{}},{"name":"stderr","text":"[rank0]:[W1118 07:04:03.377694046 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:04:04 [utils.py:233] non-default args: {'max_num_batched_tokens': 2048, 'disable_log_stats': True, 'model': 'facebook/opt-125m'}\nINFO 11-18 07:04:04 [model.py:547] Resolved architecture: OPTForCausalLM\nINFO 11-18 07:04:04 [model.py:1510] Using max model len 2048\nINFO 11-18 07:04:04 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-18 07:04:10.042433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763449450.065344    8234 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763449450.073067    8234 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:04:16 [__init__.py:216] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:17 [core.py:644] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:17 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m ERROR 11-18 07:04:19 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n","output_type":"stream"},{"name":"stderr","text":"[W1118 07:04:29.885635381 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W1118 07:04:39.896289603 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:39 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m WARNING 11-18 07:04:39 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:39 [gpu_model_runner.py:2602] Starting to load model facebook/opt-125m...\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:40 [gpu_model_runner.py:2634] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:40 [cuda.py:372] Using FlexAttention backend on V1 engine.\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:40 [weight_utils.py:392] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n","output_type":"stream"},{"name":"stderr","text":"Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:41 [default_loader.py:267] Loading weights took 0.28 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:41 [gpu_model_runner.py:2653] Model loading took 0.2393 GiB and 0.826752 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:44 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/932ffef25e/rank_0_0/backbone for vLLM's torch.compile\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:44 [backends.py:559] Dynamo bytecode transform time: 2.42 s\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:45 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.286 s\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:45 [monitor.py:34] torch.compile takes 2.42 s in total\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:46 [gpu_worker.py:298] Available KV cache memory: 12.53 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:46 [kv_cache_utils.py:1087] GPU KV cache size: 364,864 tokens\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:46 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 178.16x\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m WARNING 11-18 07:04:46 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 61.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:48 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.19 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8234)\u001b[0;0m INFO 11-18 07:04:48 [core.py:210] init engine (profile, create kv cache, warmup model) took 6.89 seconds\nINFO 11-18 07:04:49 [llm.py:306] Supported_tasks: ['generate']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4405ef6caae0430eb766827cba0993ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c465ed485584f6dbf49d35feb31cbb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d9ea4438cda422a96225c2f25395d0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/50 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"080689ddd17647229bc2a676f290950f"}},"metadata":{}},{"name":"stderr","text":"[rank0]:[W1118 07:04:57.833030545 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:04:59 [utils.py:233] non-default args: {'max_num_batched_tokens': 2048, 'disable_log_stats': True, 'model': 'facebook/opt-125m'}\nINFO 11-18 07:04:59 [model.py:547] Resolved architecture: OPTForCausalLM\nINFO 11-18 07:04:59 [model.py:1510] Using max model len 2048\nINFO 11-18 07:04:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-18 07:05:04.912437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763449504.935539    8323 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763449504.943544    8323 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:05:11 [__init__.py:216] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:12 [core.py:644] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:12 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m ERROR 11-18 07:05:14 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n","output_type":"stream"},{"name":"stderr","text":"[W1118 07:05:24.748137219 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W1118 07:05:34.758952082 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:34 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m WARNING 11-18 07:05:34 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:34 [gpu_model_runner.py:2602] Starting to load model facebook/opt-125m...\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:35 [gpu_model_runner.py:2634] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:35 [cuda.py:372] Using FlexAttention backend on V1 engine.\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:35 [weight_utils.py:392] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n","output_type":"stream"},{"name":"stderr","text":"Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.62it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.61it/s]\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:35 [default_loader.py:267] Loading weights took 0.28 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:36 [gpu_model_runner.py:2653] Model loading took 0.2393 GiB and 0.767125 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:39 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/932ffef25e/rank_0_0/backbone for vLLM's torch.compile\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:39 [backends.py:559] Dynamo bytecode transform time: 2.45 s\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:40 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.290 s\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:40 [monitor.py:34] torch.compile takes 2.45 s in total\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:41 [gpu_worker.py:298] Available KV cache memory: 12.53 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:41 [kv_cache_utils.py:1087] GPU KV cache size: 364,864 tokens\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:41 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 178.16x\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m WARNING 11-18 07:05:41 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 62.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:43 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.19 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8323)\u001b[0;0m INFO 11-18 07:05:43 [core.py:210] init engine (profile, create kv cache, warmup model) took 6.93 seconds\nINFO 11-18 07:05:44 [llm.py:306] Supported_tasks: ['generate']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce1913568235479d9cc1e063a2c8bf27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fbc2d34d51446c8b365430621b22ba4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0914f4c725434b57ad7048dca9d62d74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/50 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc158e600f5c46529c5fba9598b89b9c"}},"metadata":{}},{"name":"stderr","text":"[rank0]:[W1118 07:05:56.501768524 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"},{"name":"stdout","text":"Running request count experiments...\nINFO 11-18 07:05:57 [utils.py:233] non-default args: {'max_num_batched_tokens': 2048, 'disable_log_stats': True, 'model': 'facebook/opt-125m'}\nINFO 11-18 07:05:57 [model.py:547] Resolved architecture: OPTForCausalLM\nINFO 11-18 07:05:57 [model.py:1510] Using max model len 2048\nINFO 11-18 07:05:57 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-18 07:06:03.249311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763449563.272089    8412 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763449563.279675    8412 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:06:09 [__init__.py:216] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:10 [core.py:644] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:10 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m ERROR 11-18 07:06:12 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n","output_type":"stream"},{"name":"stderr","text":"[W1118 07:06:22.051050995 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W1118 07:06:32.061707194 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:32 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m WARNING 11-18 07:06:33 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:33 [gpu_model_runner.py:2602] Starting to load model facebook/opt-125m...\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:33 [gpu_model_runner.py:2634] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:33 [cuda.py:372] Using FlexAttention backend on V1 engine.\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:33 [weight_utils.py:392] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n","output_type":"stream"},{"name":"stderr","text":"Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:34 [default_loader.py:267] Loading weights took 0.28 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:35 [gpu_model_runner.py:2653] Model loading took 0.2393 GiB and 0.758835 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:37 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/932ffef25e/rank_0_0/backbone for vLLM's torch.compile\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:37 [backends.py:559] Dynamo bytecode transform time: 2.43 s\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:38 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.287 s\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:38 [monitor.py:34] torch.compile takes 2.43 s in total\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:39 [gpu_worker.py:298] Available KV cache memory: 12.53 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:40 [kv_cache_utils.py:1087] GPU KV cache size: 364,864 tokens\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:40 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 178.16x\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m WARNING 11-18 07:06:40 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 62.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:41 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.19 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8412)\u001b[0;0m INFO 11-18 07:06:41 [core.py:210] init engine (profile, create kv cache, warmup model) took 6.94 seconds\nINFO 11-18 07:06:42 [llm.py:306] Supported_tasks: ['generate']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28d19dc1deae4ebabd0bed4a4a0486ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196cecf5b8fa4ac198211541e2359d62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50e182bb0416482f982e08c0db91e3bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23181023e887408693be57ca42dcf623"}},"metadata":{}},{"name":"stderr","text":"[rank0]:[W1118 07:06:50.178353701 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:06:52 [utils.py:233] non-default args: {'max_num_batched_tokens': 2048, 'disable_log_stats': True, 'model': 'facebook/opt-125m'}\nINFO 11-18 07:06:52 [model.py:547] Resolved architecture: OPTForCausalLM\nINFO 11-18 07:06:52 [model.py:1510] Using max model len 2048\nINFO 11-18 07:06:52 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-18 07:06:58.161247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763449618.184488    8501 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763449618.192179    8501 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:07:04 [__init__.py:216] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:05 [core.py:644] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:05 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m ERROR 11-18 07:07:07 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n","output_type":"stream"},{"name":"stderr","text":"[W1118 07:07:17.065260943 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W1118 07:07:27.076145303 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:27 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m WARNING 11-18 07:07:28 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:28 [gpu_model_runner.py:2602] Starting to load model facebook/opt-125m...\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:28 [gpu_model_runner.py:2634] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:28 [cuda.py:372] Using FlexAttention backend on V1 engine.\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:28 [weight_utils.py:392] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n","output_type":"stream"},{"name":"stderr","text":"Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.61it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.61it/s]\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:29 [default_loader.py:267] Loading weights took 0.28 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:30 [gpu_model_runner.py:2653] Model loading took 0.2393 GiB and 0.880920 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:33 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/932ffef25e/rank_0_0/backbone for vLLM's torch.compile\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:33 [backends.py:559] Dynamo bytecode transform time: 2.45 s\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:33 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.299 s\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:33 [monitor.py:34] torch.compile takes 2.45 s in total\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:34 [gpu_worker.py:298] Available KV cache memory: 12.53 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:35 [kv_cache_utils.py:1087] GPU KV cache size: 364,864 tokens\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:35 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 178.16x\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m WARNING 11-18 07:07:35 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 62.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:37 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.19 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8501)\u001b[0;0m INFO 11-18 07:07:37 [core.py:210] init engine (profile, create kv cache, warmup model) took 6.95 seconds\nINFO 11-18 07:07:38 [llm.py:306] Supported_tasks: ['generate']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a229b5f3ca8427386ef936adadcd4bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b26d6e0c1b1424c9ec3cea52594bd01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b1f6a9f4ae47d5b2873af67a9b2f8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/50 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"136159cc9c1f4145b1ef17539db56aad"}},"metadata":{}},{"name":"stderr","text":"[rank0]:[W1118 07:07:49.082965363 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:07:51 [utils.py:233] non-default args: {'max_num_batched_tokens': 2048, 'disable_log_stats': True, 'model': 'facebook/opt-125m'}\nINFO 11-18 07:07:52 [model.py:547] Resolved architecture: OPTForCausalLM\nINFO 11-18 07:07:52 [model.py:1510] Using max model len 2048\nINFO 11-18 07:07:52 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-18 07:07:57.640620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763449677.664006    8590 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763449677.671609    8590 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:08:03 [__init__.py:216] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:05 [core.py:644] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:05 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m ERROR 11-18 07:08:06 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n","output_type":"stream"},{"name":"stderr","text":"[W1118 07:08:17.470442149 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W1118 07:08:27.481195701 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:27 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m WARNING 11-18 07:08:27 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:27 [gpu_model_runner.py:2602] Starting to load model facebook/opt-125m...\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:27 [gpu_model_runner.py:2634] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:27 [cuda.py:372] Using FlexAttention backend on V1 engine.\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:28 [weight_utils.py:392] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n","output_type":"stream"},{"name":"stderr","text":"Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:28 [default_loader.py:267] Loading weights took 0.28 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:29 [gpu_model_runner.py:2653] Model loading took 0.2393 GiB and 0.774182 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:32 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/932ffef25e/rank_0_0/backbone for vLLM's torch.compile\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:32 [backends.py:559] Dynamo bytecode transform time: 2.41 s\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:32 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.290 s\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:33 [monitor.py:34] torch.compile takes 2.41 s in total\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:34 [gpu_worker.py:298] Available KV cache memory: 12.53 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:34 [kv_cache_utils.py:1087] GPU KV cache size: 364,864 tokens\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:34 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 178.16x\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m WARNING 11-18 07:08:34 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 63.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:36 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.19 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8590)\u001b[0;0m INFO 11-18 07:08:36 [core.py:210] init engine (profile, create kv cache, warmup model) took 6.85 seconds\nINFO 11-18 07:08:37 [llm.py:306] Supported_tasks: ['generate']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"537df190077b4a3d9ddc141a7d9ed668"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"573183c88aee4237917b4abacea61b40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b8152d82b3c4ec7a17cb5c047a8d56e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a61cee784aa48d7813ea0c5d54a2add"}},"metadata":{}},{"name":"stderr","text":"[rank0]:[W1118 07:08:57.552513379 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:09:00 [utils.py:233] non-default args: {'max_num_batched_tokens': 2048, 'disable_log_stats': True, 'model': 'facebook/opt-125m'}\nINFO 11-18 07:09:00 [model.py:547] Resolved architecture: OPTForCausalLM\nINFO 11-18 07:09:00 [model.py:1510] Using max model len 2048\nINFO 11-18 07:09:00 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-18 07:09:06.359321: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763449746.383026    8679 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763449746.390753    8679 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:09:12 [__init__.py:216] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:14 [core.py:644] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:14 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m ERROR 11-18 07:09:15 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n","output_type":"stream"},{"name":"stderr","text":"[W1118 07:09:25.226413113 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W1118 07:09:35.237144579 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:35 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m WARNING 11-18 07:09:36 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:36 [gpu_model_runner.py:2602] Starting to load model facebook/opt-125m...\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:36 [gpu_model_runner.py:2634] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:36 [cuda.py:372] Using FlexAttention backend on V1 engine.\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:37 [weight_utils.py:392] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n","output_type":"stream"},{"name":"stderr","text":"Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.62it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.61it/s]\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:37 [default_loader.py:267] Loading weights took 0.29 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:38 [gpu_model_runner.py:2653] Model loading took 0.2393 GiB and 0.771048 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:41 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/932ffef25e/rank_0_0/backbone for vLLM's torch.compile\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:41 [backends.py:559] Dynamo bytecode transform time: 2.44 s\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:41 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.292 s\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:41 [monitor.py:34] torch.compile takes 2.44 s in total\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:42 [gpu_worker.py:298] Available KV cache memory: 12.53 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:43 [kv_cache_utils.py:1087] GPU KV cache size: 364,864 tokens\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:43 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 178.16x\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m WARNING 11-18 07:09:43 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 62.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:45 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.19 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8679)\u001b[0;0m INFO 11-18 07:09:45 [core.py:210] init engine (profile, create kv cache, warmup model) took 6.95 seconds\nINFO 11-18 07:09:45 [llm.py:306] Supported_tasks: ['generate']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6560c3fdee245d4a11969faecacafd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcb20fe179e1447fa34ce7289dc1566c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6474771796744eb988ecbcfce03690a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1865c65005f4499ad465c2df03eebc2"}},"metadata":{}},{"name":"stderr","text":"[rank0]:[W1118 07:10:18.783044149 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:10:25 [utils.py:233] non-default args: {'max_num_batched_tokens': 2048, 'disable_log_stats': True, 'model': 'facebook/opt-125m'}\nINFO 11-18 07:10:25 [model.py:547] Resolved architecture: OPTForCausalLM\nINFO 11-18 07:10:25 [model.py:1510] Using max model len 2048\nINFO 11-18 07:10:25 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-18 07:10:31.214035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763449831.237091    8768 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763449831.244741    8768 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n","output_type":"stream"},{"name":"stdout","text":"INFO 11-18 07:10:37 [__init__.py:216] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:10:39 [core.py:644] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:10:39 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m ERROR 11-18 07:10:40 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n","output_type":"stream"},{"name":"stderr","text":"[W1118 07:10:50.120223089 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W1118 07:11:00.130929409 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:00 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m WARNING 11-18 07:11:01 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:01 [gpu_model_runner.py:2602] Starting to load model facebook/opt-125m...\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:01 [gpu_model_runner.py:2634] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:01 [cuda.py:372] Using FlexAttention backend on V1 engine.\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:01 [weight_utils.py:392] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n","output_type":"stream"},{"name":"stderr","text":"Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.58it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.58it/s]\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:02 [default_loader.py:267] Loading weights took 0.29 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:03 [gpu_model_runner.py:2653] Model loading took 0.2393 GiB and 0.761851 seconds\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:05 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/932ffef25e/rank_0_0/backbone for vLLM's torch.compile\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:05 [backends.py:559] Dynamo bytecode transform time: 2.42 s\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:06 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.288 s\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:06 [monitor.py:34] torch.compile takes 2.42 s in total\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:07 [gpu_worker.py:298] Available KV cache memory: 12.53 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:08 [kv_cache_utils.py:1087] GPU KV cache size: 364,864 tokens\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:08 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 178.16x\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m WARNING 11-18 07:11:08 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 61.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:10 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.19 GiB\n\u001b[1;36m(EngineCore_DP0 pid=8768)\u001b[0;0m INFO 11-18 07:11:10 [core.py:210] init engine (profile, create kv cache, warmup model) took 6.93 seconds\nINFO 11-18 07:11:10 [llm.py:306] Supported_tasks: ['generate']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0f968ee5e5496b9580514852c5510a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0472b142621442e9963f77d0b4208bc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dce3748131044aa4964fb0452742734a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/500 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb03cb7f807142828f27f86a2dcd766a"}},"metadata":{}},{"name":"stderr","text":"[rank0]:[W1118 07:12:24.190660733 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 3 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"print(batch_results)\nprint(length_results)\nprint(request_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T07:12:49.232307Z","iopub.execute_input":"2025-11-18T07:12:49.233001Z","iopub.status.idle":"2025-11-18T07:12:49.242358Z","shell.execute_reply.started":"2025-11-18T07:12:49.232977Z","shell.execute_reply":"2025-11-18T07:12:49.241549Z"}},"outputs":[{"name":"stdout","text":"   batch_size   throughput  elapsed_time\n0        1024  1481.249009      8.641356\n1        2048  1584.172501      8.079928\n   sequence_length   throughput  elapsed_time\n0               32   862.707103      3.709254\n1               64  1352.743424      4.731126\n2              128  1559.473363      8.207899\n   num_requests   throughput  elapsed_time\n0            10   552.875013      4.630341\n1            50  1553.687925      8.238463\n2           100  1543.557289     16.585066\n3           200  1768.359138     28.953395\n4           500  1813.848725     70.568178\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"  - Write a clear analysis of the results (2 points)\n","metadata":{"id":"iWZu4NdEx32F"}},{"cell_type":"markdown","source":"2.3.3 Experiments and Analysis\n\nParameter Studies\n\nBatch size experiments: To vary the batch size, I just varied the max_num_batched_tokens argument passed when intializing the LLM. Since the sequence length was set to 128 for input and output in the batch size experiments, when max_num_batched_tokens = 1024 then the number of requests the LLM processes together is 1024 / 128 = 8 and when max_num_batched_tokens = 2048 then the number of requests the LLM processes together is 2048 / 128 = 16. From the above visualization of batch size vs. throughput and the above display of the dataframe for the batch size experiment results, we can see that for the two batch sizes tested(1024, 2048) the throughput increases slightly. Maybe the throughput will increase more rapidly for larger batch sizes but unfortunately whenever I tested larger batch sizes, I kept getting Out of Memory errors.\n\nInput/output length experiments: From the above visualization of sequence length vs. throughput and the above display of the dataframe for the sequence length experiment results, we can clearly see that throughput increases with sequence length which means that the GPUs are getting utilized better as the sequence length increases as well. Although the elapsed time of each run increases as sequence length increases due to the LLM having to handle more tokens, the GPU is utilized more efficiently as seen in the increases in throughput.\n\nRequest count experiments:  From the above visualization of number of requests vs. throughput and the above display of the dataframe for the request count experiment results, we can clearly see that throughput generally improves as the number of requests increases. However, it's also clearly evident that the throughput increase slows down after number of requests = 50. So the LLM definitely is able to scale efficiently with more requests up to a certain point after which the throughput gains kind of slow down since the GPUs are probably already being fully utilized.","metadata":{"id":"88i7p_nJxlJO"}}]}