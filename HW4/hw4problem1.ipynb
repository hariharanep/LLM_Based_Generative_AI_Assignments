{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13745668,"sourceType":"datasetVersion","datasetId":8746485},{"sourceId":13746111,"sourceType":"datasetVersion","datasetId":8746812}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tune a Generative AI Model for Dialogue Summarization","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics.","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents","metadata":{}},{"cell_type":"markdown","source":"- [ 1 - Load Required Dependencies, Dataset and LLM](#1)\n  - [ 1.1 - Set up Required Dependencies](#1.1)\n  - [ 1.2 - Load Dataset and LLM](#1.2)\n  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n- [ 2 - Perform Full Fine-Tuning](#2)\n  - [ 2.1 - Preprocess the Dialog-Summary Dataset](#2.1)\n  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n  - [ 2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n  - [ 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n- [ 3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n  - [ 3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n  - [ 3.2 - Train PEFT Adapter](#3.2)\n  - [ 3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n  - [ 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)","metadata":{}},{"cell_type":"markdown","source":"<a name='1'></a>\n## 1 - Load Required Dependencies, Dataset and LLM (5 points)","metadata":{}},{"cell_type":"markdown","source":"<a name='1.1'></a>\n### 1.1 - Set up Required Dependencies (1 point)","metadata":{}},{"cell_type":"markdown","source":"Now install the required packages for the LLM and datasets.\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5JZ25vcmUgdGhlIHdhcm5pbmdzIGFuZCBlcnJvcnMsIGFsb25nIHdpdGggdGhlIG5vdGUgYWJvdXQgcmVzdGFydGluZyB0aGUga2VybmVsIGF0IHRoZSBlbmQuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>","metadata":{}},{"cell_type":"code","source":"# Installing required dependencies\n!pip install datasets torch transformers evaluate rouge_score loralib peft wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T16:45:39.667289Z","iopub.execute_input":"2025-11-15T16:45:39.667917Z","iopub.status.idle":"2025-11-15T16:45:43.355971Z","shell.execute_reply.started":"2025-11-15T16:45:39.667893Z","shell.execute_reply":"2025-11-15T16:45:43.355215Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\nRequirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: loralib in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.20.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.2)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.1.3)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.9.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.3.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.5.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (6.33.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.12.4)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{}},{"cell_type":"markdown","source":"Import the necessary components. Some of them are new for this week, they will be discussed later in the notebook.","metadata":{}},{"cell_type":"code","source":"# Importing necessary components\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom transformers import Trainer, TrainingArguments\nimport evaluate\nimport torch\nimport time\nimport wandb\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:08:33.683673Z","iopub.execute_input":"2025-11-15T18:08:33.684505Z","iopub.status.idle":"2025-11-15T18:08:33.688258Z","shell.execute_reply.started":"2025-11-15T18:08:33.684478Z","shell.execute_reply":"2025-11-15T18:08:33.687509Z"}},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":"<a name='1.2'></a>\n### 1.2 - Load Dataset and LLM (2 points)\n\nYou are going to continue experimenting with the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics.","metadata":{}},{"cell_type":"code","source":"# Loading Dataset\nhuggingface_dataset_name = \"knkarthick/dialogsum\"\ndataset = load_dataset(huggingface_dataset_name)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:49:27.552041Z","iopub.execute_input":"2025-11-15T15:49:27.552423Z","iopub.status.idle":"2025-11-15T15:49:29.913141Z","shell.execute_reply.started":"2025-11-15T15:49:27.552397Z","shell.execute_reply":"2025-11-15T15:49:29.912529Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebebd841b1c14c7895f2f6a60079fadc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a92cbbe72704662937d034c31cff461"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f868c7c34eff455e96ee1f09fb1e44f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1a57849acd946388c41bcc514633e06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"788c07d975fc4c28ba6cb68bec49a706"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0624ebae5f384390ba254b7087f31fbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f7ee2df44234ca99104b87abb4159c6"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"Load the pre-trained [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer directly from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-small) of FLAN-T5. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model.","metadata":{}},{"cell_type":"code","source":"# Loading pre-trained FLAN-T5 small model and its tokenizer directly from HuggingFace\nmodel_name = \"google/flan-t5-small\"\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:52:07.535574Z","iopub.execute_input":"2025-11-15T15:52:07.536216Z","iopub.status.idle":"2025-11-15T15:52:10.625735Z","shell.execute_reply.started":"2025-11-15T15:52:07.536191Z","shell.execute_reply":"2025-11-15T15:52:10.625132Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad839baa6f274a1fb2c98de58903609a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dcd7c6b81424c4782b1096260c94d45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3fa2b1f07114beb9e97b58a2fc611eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d6ac33881fe417381073ccf39715093"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8ff65cbb2bf4fe496518e118cbb6e01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5beb3dab4fba42a09ddf5ee347439797"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99375f201f1b4a4d8268f52909a8b24c"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it.","metadata":{}},{"cell_type":"code","source":"# Function to print number of parameters in model and number of parameters in model that are trainable\ndef print_number_of_trainable_model_parameters(model):\n    trainable_model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    all_model_params = sum(p.numel() for p in model.parameters())\n    print(\"Total Number of Parameters: \" + str(all_model_params))\n    print(\"Total Number of Trainable Parameters: \" + str(trainable_model_params))\n\nprint_number_of_trainable_model_parameters(original_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:52:15.151665Z","iopub.execute_input":"2025-11-15T15:52:15.151984Z","iopub.status.idle":"2025-11-15T15:52:15.158560Z","shell.execute_reply.started":"2025-11-15T15:52:15.151961Z","shell.execute_reply":"2025-11-15T15:52:15.157920Z"}},"outputs":[{"name":"stdout","text":"Total Number of Parameters: 76961152\nTotal Number of Trainable Parameters: 76961152\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"<a name='1.3'></a>\n### 1.3 - Test the Model with Zero Shot Inferencing (2 Points)\n\nTest the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand.","metadata":{}},{"cell_type":"code","source":"# Get random dialogue and it's summary from the test dataset\nindex = 200\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\n# Create prompt for zero shot inferencing\nprompt = \"Summarize the following dialogue.\\n\\n\" + dialogue + \"\\n\\nSummary:\"\n\n# Tokenize prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# Get model to generate a response to input prompt\nresponse = original_model.generate(**inputs)\n\n# Decode model's response\noutput = tokenizer.decode(\n    response[0],\n    skip_special_tokens=True\n)\n\n# Compare zero shot inferencing output of our model to baseline human summary\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:56:59.716877Z","iopub.execute_input":"2025-11-15T15:56:59.717186Z","iopub.status.idle":"2025-11-15T15:57:00.470705Z","shell.execute_reply.started":"2025-11-15T15:56:59.717164Z","shell.execute_reply":"2025-11-15T15:57:00.469873Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nSummarize the following dialogue.\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nSummary:\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nYou'd like to add a CD-ROM drive to your software.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"1.3\n\nCompare the generated summary with the human baseline using qualitative analysis\n\nAs seen in the above output, the zero-shot generated summary does capture the information that appears towards the end of the conversation, but fails to capture the main purpose of the conversation as a whole like the baseline human summary does. Also, the zero-shot generated summary thinks Person 2 is me instead of a third party conversation between two random people.","metadata":{}},{"cell_type":"markdown","source":"<a name='2'></a>\n## 2 - Perform Full Fine-Tuning (10 points)","metadata":{}},{"cell_type":"markdown","source":"<a name='2.1'></a>\n### 2.1 - Preprocess the Dialog-Summary Dataset (2 points)\n\nYou need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following conversation` and to the start of the summary with `Summary` as follows:\n\nTraining prompt (dialogue):\n```\nSummarize the following conversation.\n\n    Chris: This is his part of the conversation.\n    Antje: This is her part of the conversation.\n    \nSummary:\n```\n\nTraining response (summary):\n```\nBoth Chris and Antje participated in the conversation.\n```\n\nThen preprocess the prompt-response dataset into tokens and pull out their `input_ids` (1 per token).","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    # Create prompt for every example in batch\n    inputs = [\"Summarize the following conversation.\\n\\n\" + dialogue + \"\\n\\nSummary:\" \n              for dialogue in examples['dialogue']]\n\n    # Tokenize the prompts\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    # Tokenize the labels(baseline human summaries)\n    labels = tokenizer(examples['summary'], max_length=128, truncation=True, padding=\"max_length\") \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    # Return tokenized model inputs\n    return model_inputs\n\n# The dataset actually contains 3 diff splits: train, validation, test.\n# The tokenize_function code is handling all data across all splits in batches.\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset['train'].column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:34:25.621661Z","iopub.execute_input":"2025-11-15T17:34:25.622375Z","iopub.status.idle":"2025-11-15T17:34:31.475937Z","shell.execute_reply.started":"2025-11-15T17:34:25.622350Z","shell.execute_reply":"2025-11-15T17:34:31.475166Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81bb21e8c7224832a1f5d30825ec9e04"}},"metadata":{}}],"execution_count":57},{"cell_type":"markdown","source":"To save some time in the lab, you will subsample the dataset:","metadata":{}},{"cell_type":"code","source":"# Create a subsampled version of the dataset for efficient training\ntokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 3 == 0, with_indices=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:35:12.760443Z","iopub.execute_input":"2025-11-15T17:35:12.760994Z","iopub.status.idle":"2025-11-15T17:35:18.896768Z","shell.execute_reply.started":"2025-11-15T17:35:12.760966Z","shell.execute_reply":"2025-11-15T17:35:18.896212Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7594e74430894125a75cc9e0675e6692"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e00371361b141138f59a6ea871e4e44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"931f243ed3fe4619b87b314cab032f94"}},"metadata":{}}],"execution_count":58},{"cell_type":"markdown","source":"Check the shapes of all three parts of the dataset:","metadata":{}},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {tokenized_datasets['train'].shape}\")\nprint(f\"Validation: {tokenized_datasets['validation'].shape}\")\nprint(f\"Test: {tokenized_datasets['test'].shape}\")\n\nprint(tokenized_datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:35:22.876847Z","iopub.execute_input":"2025-11-15T17:35:22.877591Z","iopub.status.idle":"2025-11-15T17:35:22.883755Z","shell.execute_reply.started":"2025-11-15T17:35:22.877567Z","shell.execute_reply":"2025-11-15T17:35:22.883005Z"}},"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (4154, 3)\nValidation: (167, 3)\nTest: (500, 3)\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 4154\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 167\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 500\n    })\n})\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"The output dataset is ready for fine-tuning.","metadata":{}},{"cell_type":"markdown","source":"<a name='2.2'></a>\n### 2.2 - Fine-Tune the Model with the Preprocessed Dataset (3 points)\n\nNow utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment.","metadata":{}},{"cell_type":"code","source":"output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n\n# Configure TrainingArguments with appropriate learning rate, epochs, and other hyperparameters\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=5e-5,\n    num_train_epochs=3,\n    auto_find_batch_size=True,\n    logging_steps=100\n)\n\n# Initialize the Hugging Face Trainer class with the model, training arguments, and datasets\ntrainer = Trainer(\n    model=original_model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:35:44.082613Z","iopub.execute_input":"2025-11-15T17:35:44.082967Z","iopub.status.idle":"2025-11-15T17:35:44.133908Z","shell.execute_reply.started":"2025-11-15T17:35:44.082943Z","shell.execute_reply":"2025-11-15T17:35:44.133072Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"Start training process...\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5Zb3UgY2FuIHNhZmVseSBpZ25vcmUgdGhlIHdhcm5pbmcgbWVzc2FnZXMuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>\n\nThe code trainer.train() utilizes the Weights & Biases (wandb) library to track and visualize the training process. To proceed, you'll need to sign up for a wandb account using your Gmail and then enter your unique API token to authenticate and enable logging of the training progress.","metadata":{}},{"cell_type":"code","source":"# Login to wandb and initialize it\nwandb.login(key = \"43d56b51a7a89074cdecf6fd4ef49d1d5256762e\")\nwandb.init(project=\"hw4_problem1\", name=\"2.2\")\n\n# Execute the training process using trainer.train()\ntrainer.train()\n\n# Save the fine-tuned model checkpoint\ntrainer.save_model(\"./flan-t5-finetuned\")\ntokenizer.save_pretrained(\"./flan-t5-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:36:29.193524Z","iopub.execute_input":"2025-11-15T17:36:29.194128Z","iopub.status.idle":"2025-11-15T17:43:44.564146Z","shell.execute_reply.started":"2025-11-15T17:36:29.194103Z","shell.execute_reply":"2025-11-15T17:43:44.563342Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251115_173629-z4f57jnq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/pkh2120-columbia-university/hw4_problem1/runs/z4f57jnq' target=\"_blank\">2.2</a></strong> to <a href='https://wandb.ai/pkh2120-columbia-university/hw4_problem1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/pkh2120-columbia-university/hw4_problem1' target=\"_blank\">https://wandb.ai/pkh2120-columbia-university/hw4_problem1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/pkh2120-columbia-university/hw4_problem1/runs/z4f57jnq' target=\"_blank\">https://wandb.ai/pkh2120-columbia-university/hw4_problem1/runs/z4f57jnq</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [780/780 07:06, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>11.767500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>7.639700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>5.739400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>5.090000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>4.869700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>4.734400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>4.730600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"('./flan-t5-finetuned/tokenizer_config.json',\n './flan-t5-finetuned/special_tokens_map.json',\n './flan-t5-finetuned/spiece.model',\n './flan-t5-finetuned/added_tokens.json',\n './flan-t5-finetuned/tokenizer.json')"},"metadata":{}}],"execution_count":62},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>\nCreate an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:","metadata":{}},{"cell_type":"code","source":"# Load tokenizer and models\n# Import T5Tokenizer from transformers\nfrom transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n\n# Define the model path using the config.json path\nmodel_path = \"./flan-t5-finetuned\"\n\n# Load tokenizer and models\n# Use the default T5 tokenizer\ninstruct_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\") # TODO  # or \"t5-base\", \"t5-large\", etc.\n\n# Load the model in a way that is compatible with single-GPU environments\ninstruct_model = AutoModelForSeq2SeqLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    # The following line addresses the multi-GPU loading issue\n    device_map=\"auto\",\n)\n\n# Move model to GPU if available (optional, as device_map=\"auto\" should handle it)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninstruct_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:44:54.696759Z","iopub.execute_input":"2025-11-15T17:44:54.697493Z","iopub.status.idle":"2025-11-15T17:44:55.439187Z","shell.execute_reply.started":"2025-11-15T17:44:54.697461Z","shell.execute_reply":"2025-11-15T17:44:55.438502Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n)"},"metadata":{}}],"execution_count":64},{"cell_type":"markdown","source":"<a name='2.3'></a>\n### 2.3 - Evaluate the Model Qualitatively (Human Evaluation) (2 points)\n\nAs with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model.","metadata":{}},{"cell_type":"code","source":"# Get random test dialogue and label from test dataset\nindex = 200\ndialogue = dataset['test'][index]['dialogue']\nhuman_baseline_summary = dataset['test'][index]['summary']\n\n# Construct prompt\nprompt = \"Summarize the following conversation.\\n\\n\" + dialogue + \"\\n\\nSummary:\"\n\n# Tokenize the prompt\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n# Move input_ids to the same device as the model\ninput_ids = input_ids.to(device)\n\n# Get text output from original model\noriginal_tokenizer = tokenizer\noriginal_model_response = original_model.generate(input_ids)\noriginal_model_outputs = original_model_response[0]\noriginal_model_text_output = original_tokenizer.decode(\n    original_model_outputs,\n    skip_special_tokens=True\n)\n\n# Get text output from instruct finetuned model\ninstruct_model_response = instruct_model.generate(input_ids)\ninstruct_model_outputs = instruct_model_response[0]\ninstruct_model_text_output = instruct_tokenizer.decode(\n    instruct_model_outputs,\n    skip_special_tokens=True\n)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:46:57.834300Z","iopub.execute_input":"2025-11-15T17:46:57.834592Z","iopub.status.idle":"2025-11-15T17:46:58.259010Z","shell.execute_reply.started":"2025-11-15T17:46:57.834569Z","shell.execute_reply":"2025-11-15T17:46:58.258334Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\nORIGINAL MODEL:\nShare this with the others.\n---------------------------------------------------------------------------------------------------\nINSTRUCT MODEL:\n#Person1# You could consider adding a painting program to your software.\n","output_type":"stream"}],"execution_count":69},{"cell_type":"markdown","source":"2.3 Compare outputs across models using the same test examples, Analyze improvements in summary quality, coherence, and relevance\n\nAs you can see in the output above, the finetuned model captures the conversation much better than the original model which ended up giving a different response to the test prompt this time. The original model doesn't really capture the purpose of the conversation well at all while the finetuned model at least captures some key points made in the conversation. ","metadata":{}},{"cell_type":"markdown","source":"<a name='2.4'></a>\n### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric) (3 points)\n\nThe [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.","metadata":{}},{"cell_type":"code","source":"# Load rouge evaluator\nrouge = evaluate.load(\"rouge\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:51:08.739690Z","iopub.execute_input":"2025-11-15T17:51:08.740429Z","iopub.status.idle":"2025-11-15T17:51:09.936198Z","shell.execute_reply.started":"2025-11-15T17:51:08.740403Z","shell.execute_reply":"2025-11-15T17:51:09.935588Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6684957354147c8a2e0095d6b2d00f2"}},"metadata":{}}],"execution_count":70},{"cell_type":"markdown","source":"Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results.","metadata":{}},{"cell_type":"code","source":"# Get 10 test set dialogues and their labels\ndialogues = dataset['test'][40:50]['dialogue']\nhuman_baseline_summaries = dataset['test'][40:50]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\n\nfor _, dialogue in enumerate(dialogues):\n    prompt = \"Summarize the following conversation.\\n\\n\" + dialogue + \"\\n\\nSummary:\"\n    # Tokenize the prompt\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    # Move input_ids to the same device as the model\n    input_ids = input_ids.to(device)\n\n    # Get text output from original model\n    original_model_response = original_model.generate(input_ids)\n    original_model_outputs = original_model_response[0]\n    original_model_text_output = original_tokenizer.decode(\n        original_model_outputs,\n        skip_special_tokens=True\n    )\n    \n    # Get text output from instruct finetuned model\n    instruct_model_response = instruct_model.generate(input_ids)\n    instruct_model_outputs = instruct_model_response[0]\n    instruct_model_text_output = instruct_tokenizer.decode(\n        instruct_model_outputs,\n        skip_special_tokens=True\n    )\n\n    # Append model outputs to appropriate lists\n    original_model_summaries.append(original_model_text_output)\n    instruct_model_summaries.append(instruct_model_text_output)\n\n# Display human baseline, original model, and finetuned model summaries in a dataframe\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:58:28.846141Z","iopub.execute_input":"2025-11-15T17:58:28.846808Z","iopub.status.idle":"2025-11-15T17:58:33.178153Z","shell.execute_reply.started":"2025-11-15T17:58:28.846785Z","shell.execute_reply":"2025-11-15T17:58:33.177382Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  #Person1# is in a hurry to catch a train. Tom ...   \n1  #Person1# is rushing to catch a train but Tom ...   \n2  #Person1# wants to adjust #Person1#'s life and...   \n3  #Person1# has a bad lifestyle. #Person2# kindl...   \n4  #Person2# hopes #Person1# will become healthy ...   \n5  #Person1# tells #Person2# that Ruojia is marri...   \n6  #Person2# is surprised to know from #Person1# ...   \n7  #Person2# is surprised that Ruojia's married. ...   \n8  #Person2# at first thinks #Person1#'s behaviou...   \n9  #Person1# plans on playing a trick to others. ...   \n\n                        original_model_summaries  \\\n0         @Person1#Person1#Person2#It's a minute   \n1       @PresidentSony_Persons are not the same.   \n2                        You should not do this.   \n3                              It's a good idea.   \n4                                      #Person1#   \n5                              #Person, #Person.   \n6                              Share your ideas.   \n7                       @Person_________________   \n8                              #Person1#Person2#   \n9  'I'm not a fan of being rude to your friends.   \n\n                            instruct_model_summaries  \n0  @Person, I'm not a fan of the ten to nine by m...  \n1  @Person, I'm not a fan of the ten to nine by m...  \n2                                          #Person1#  \n3                                          #Person1#  \n4                                          #Person1#  \n5       #Person! #Person! #Person! #Person! #Person!  \n6       #Person! #Person! #Person! #Person! #Person!  \n7       #Person! #Person! #Person! #Person! #Person!  \n8            #Person1# You might make a few enemies.  \n9            #Person1# You might make a few enemies.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>instruct_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#Person1# is in a hurry to catch a train. Tom ...</td>\n      <td>@Person1#Person1#Person2#It's a minute</td>\n      <td>@Person, I'm not a fan of the ten to nine by m...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#Person1# is rushing to catch a train but Tom ...</td>\n      <td>@PresidentSony_Persons are not the same.</td>\n      <td>@Person, I'm not a fan of the ten to nine by m...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#Person1# wants to adjust #Person1#'s life and...</td>\n      <td>You should not do this.</td>\n      <td>#Person1#</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person1# has a bad lifestyle. #Person2# kindl...</td>\n      <td>It's a good idea.</td>\n      <td>#Person1#</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# hopes #Person1# will become healthy ...</td>\n      <td>#Person1#</td>\n      <td>#Person1#</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person1# tells #Person2# that Ruojia is marri...</td>\n      <td>#Person, #Person.</td>\n      <td>#Person! #Person! #Person! #Person! #Person!</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person2# is surprised to know from #Person1# ...</td>\n      <td>Share your ideas.</td>\n      <td>#Person! #Person! #Person! #Person! #Person!</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person2# is surprised that Ruojia's married. ...</td>\n      <td>@Person_________________</td>\n      <td>#Person! #Person! #Person! #Person! #Person!</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person2# at first thinks #Person1#'s behaviou...</td>\n      <td>#Person1#Person2#</td>\n      <td>#Person1# You might make a few enemies.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# plans on playing a trick to others. ...</td>\n      <td>'I'm not a fan of being rude to your friends.</td>\n      <td>#Person1# You might make a few enemies.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":77},{"cell_type":"markdown","source":"Evaluate the models computing ROUGE metrics. Notice the improvement in the results!","metadata":{}},{"cell_type":"code","source":"# Get rouge score metric for original model generated summaries\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries, references=human_baseline_summaries\n)\n\n# Get rouge score metric for finetuned model generated summaries\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries, references=human_baseline_summaries\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:58:59.571101Z","iopub.execute_input":"2025-11-15T17:58:59.571601Z","iopub.status.idle":"2025-11-15T17:58:59.843760Z","shell.execute_reply.started":"2025-11-15T17:58:59.571577Z","shell.execute_reply":"2025-11-15T17:58:59.843168Z"}},"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.10210084033613445, 'rouge2': 0.0, 'rougeL': 0.08015390064071605, 'rougeLsum': 0.07840561511961107}\nINSTRUCT MODEL:\n{'rouge1': 0.11086793517633425, 'rouge2': 0.0, 'rougeL': 0.09812110581030985, 'rougeLsum': 0.09455990521235702}\n","output_type":"stream"}],"execution_count":79},{"cell_type":"markdown","source":"2.4 Analyze and compare performance metrics between models\n\nAs you can clearly see from the output above, the finetuned model slightly beats the original model in every rouge metric except rouge2 which is 0.0 for both the original and finetuned model generated summaries. When you analyze the generated summaries qualitatively the outputs from the original model and finetuned model both look pretty bad though. The original model I guess allows for a more creative output than the finetuned model which is maybe why the original model generated summaries that have more of a diverse vocabulary.","metadata":{}},{"cell_type":"markdown","source":"The file `data/dialogue-summary-training-results.csv` contains a pre-populated list of all model results which you can use to evaluate on a larger section of data. Let's do that for each of the models:","metadata":{}},{"cell_type":"code","source":"# Update the path to the CSV file\nresults_path = \"../input/pre-populated-list/dialogue-summary-training-results.csv\"\nresults = pd.read_csv(results_path)\n\nhuman_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\n\n# Get rouge score metric for original model generated summaries\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries, references=human_baseline_summaries\n)\n\n# Get rouge score metric for finetuned model generated summaries\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries, references=human_baseline_summaries\n)\n\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:07:45.242743Z","iopub.execute_input":"2025-11-15T18:07:45.243120Z","iopub.status.idle":"2025-11-15T18:07:47.423549Z","shell.execute_reply.started":"2025-11-15T18:07:45.243097Z","shell.execute_reply":"2025-11-15T18:07:47.422932Z"}},"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.2216686882994889, 'rouge2': 0.0707492488737373, 'rougeL': 0.19245630286595683, 'rougeLsum': 0.192409231638204}\nINSTRUCT MODEL:\n{'rouge1': 0.4041959932817219, 'rouge2': 0.17064828985299663, 'rougeL': 0.3267557101191949, 'rougeLsum': 0.3266766725171105}\n","output_type":"stream"}],"execution_count":80},{"cell_type":"markdown","source":"The results show substantial improvement in all ROUGE metrics:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(instruct_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:08:50.980918Z","iopub.execute_input":"2025-11-15T18:08:50.981166Z","iopub.status.idle":"2025-11-15T18:08:50.987600Z","shell.execute_reply.started":"2025-11-15T18:08:50.981150Z","shell.execute_reply":"2025-11-15T18:08:50.987057Z"}},"outputs":[{"name":"stdout","text":"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\nrouge1: 18.25%\nrouge2: 9.99%\nrougeL: 13.43%\nrougeLsum: 13.43%\n","output_type":"stream"}],"execution_count":83},{"cell_type":"markdown","source":"2.4 Analyze and compare performance metrics between models\n\nAs you can clearly see from the output above, when evaluated across a larger section of data, the finetuned model beats the original model in every rouge metric with a much wider margin than previously seen when comparing with only 10 test samples.","metadata":{}},{"cell_type":"markdown","source":"<a name='3'></a>\n## 3 - Perform Parameter Efficient Fine-Tuning (PEFT) (10 points)\n\nNow, let's perform **Parameter Efficient Fine-Tuning (PEFT)** fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon.\n\nPEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained LoRA adapter emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n\nThat said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases.","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning (2 points)\n\nYou need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\n# Configure LoRA parameters using LoraConfig with appropriate rank, alpha, and target modules\nlora_config = LoraConfig(\n    r = 16, # TODO\n    lora_alpha=32,            \n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:16:40.104346Z","iopub.execute_input":"2025-11-15T18:16:40.105113Z","iopub.status.idle":"2025-11-15T18:16:40.109148Z","shell.execute_reply.started":"2025-11-15T18:16:40.105086Z","shell.execute_reply":"2025-11-15T18:16:40.108394Z"}},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":"Add LoRA adapter layers/parameters to the original LLM to be trained.","metadata":{}},{"cell_type":"code","source":"# Initialize the PEFT model \npeft_model = get_peft_model(original_model, lora_config)\n\n# Verify the reduction in trainable parameters compared to full fine tuning\nprint_number_of_trainable_model_parameters(peft_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:17:07.087360Z","iopub.execute_input":"2025-11-15T18:17:07.088010Z","iopub.status.idle":"2025-11-15T18:17:07.170144Z","shell.execute_reply.started":"2025-11-15T18:17:07.087977Z","shell.execute_reply":"2025-11-15T18:17:07.169337Z"}},"outputs":[{"name":"stdout","text":"Total Number of Parameters: 77649280\nTotal Number of Trainable Parameters: 688128\n","output_type":"stream"}],"execution_count":86},{"cell_type":"markdown","source":"<a name='3.2'></a>\n### 3.2 - Train PEFT Adapter (3 points)\n\nDefine training arguments and create `Trainer` instance.","metadata":{}},{"cell_type":"code","source":"output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n\n# Set up training arguments specific to PEFT, including higher learning rate\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-3,\n    num_train_epochs=3,\n    auto_find_batch_size=True,\n    logging_steps=100\n)\n\n# Initialize training using the Hugging Face Trainer\npeft_trainer = Trainer(\n    model=peft_model,\n    args=peft_training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:22:39.373766Z","iopub.execute_input":"2025-11-15T18:22:39.374452Z","iopub.status.idle":"2025-11-15T18:22:39.432802Z","shell.execute_reply.started":"2025-11-15T18:22:39.374425Z","shell.execute_reply":"2025-11-15T18:22:39.432031Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":87},{"cell_type":"markdown","source":"Now everything is ready to train the PEFT adapter and save the model.\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>","metadata":{}},{"cell_type":"code","source":"# Login to wandb and initialize it\nwandb.init(project=\"hw4_problem1\", name=\"3.2\")\n\npeft_model_path=\"./flan-t5-peft-finetuned\"\n\n# Execute the training process using peft_trainer.train()\npeft_trainer.train()\n\n# Save the fine-tuned model checkpoint\npeft_trainer.save_model(peft_model_path)\nperf_tokenizer = original_tokenizer\nperf_tokenizer.save_pretrained(peft_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:27:57.130106Z","iopub.execute_input":"2025-11-15T18:27:57.130418Z","iopub.status.idle":"2025-11-15T18:34:20.998875Z","shell.execute_reply.started":"2025-11-15T18:27:57.130396Z","shell.execute_reply":"2025-11-15T18:34:20.998280Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to 'default'."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/grad_norm</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>2316567469621248.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>780</td></tr><tr><td>train/grad_norm</td><td>15.75</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>4.7306</td></tr><tr><td>train_loss</td><td>6.20044</td></tr><tr><td>train_runtime</td><td>427.1481</td></tr><tr><td>train_samples_per_second</td><td>29.175</td></tr><tr><td>train_steps_per_second</td><td>1.826</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">2.2</strong> at: <a href='https://wandb.ai/pkh2120-columbia-university/hw4_problem1/runs/z4f57jnq' target=\"_blank\">https://wandb.ai/pkh2120-columbia-university/hw4_problem1/runs/z4f57jnq</a><br> View project at: <a href='https://wandb.ai/pkh2120-columbia-university/hw4_problem1' target=\"_blank\">https://wandb.ai/pkh2120-columbia-university/hw4_problem1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251115_173629-z4f57jnq/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251115_182757-1lupmg03</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/pkh2120-columbia-university/hw4_problem1/runs/1lupmg03' target=\"_blank\">3.2</a></strong> to <a href='https://wandb.ai/pkh2120-columbia-university/hw4_problem1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/pkh2120-columbia-university/hw4_problem1' target=\"_blank\">https://wandb.ai/pkh2120-columbia-university/hw4_problem1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/pkh2120-columbia-university/hw4_problem1/runs/1lupmg03' target=\"_blank\">https://wandb.ai/pkh2120-columbia-university/hw4_problem1/runs/1lupmg03</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [780/780 06:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.149200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.164500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.990500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.906600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.856700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.826900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.811200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"('./flan-t5-peft-finetuned/tokenizer_config.json',\n './flan-t5-peft-finetuned/special_tokens_map.json',\n './flan-t5-peft-finetuned/spiece.model',\n './flan-t5-peft-finetuned/added_tokens.json',\n './flan-t5-peft-finetuned/tokenizer.json')"},"metadata":{}}],"execution_count":88},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{}},{"cell_type":"markdown","source":"That training was performed on a subset of data. To load a fully trained PEFT model, read a checkpoint of a PEFT model from Google Drive.","metadata":{}},{"cell_type":"markdown","source":"Prepare this model by adding an adapter to the original FLAN-T5 model. You are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set `is_trainable=True`.","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\n# Get PeftConfig from peft_model_path\npeft_config = PeftConfig.from_pretrained(peft_model_path)\n\n# Get base peft model from peft_config\npeft_model_base = AutoModelForSeq2SeqLM.from_pretrained(peft_config.base_model_name_or_path) # TODO\n# Get tokenizer from peft_model_path\npeft_tokenizer = AutoTokenizer.from_pretrained(peft_model_path) # TODO\n\n# Generate peft_model\npeft_model = PeftModel.from_pretrained(\n    peft_model_base, \n    \"./flan-t5-peft-finetuned\",\n    is_trainable=False  # For inference only\n)\n\n\n# Move the entire peft_model to the device\npeft_model = peft_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:55:14.030083Z","iopub.execute_input":"2025-11-15T18:55:14.030766Z","iopub.status.idle":"2025-11-15T18:55:14.698972Z","shell.execute_reply.started":"2025-11-15T18:55:14.030740Z","shell.execute_reply":"2025-11-15T18:55:14.698386Z"}},"outputs":[],"execution_count":94},{"cell_type":"markdown","source":"The number of trainable parameters will be `0` due to `is_trainable=False` setting:","metadata":{}},{"cell_type":"code","source":"print_number_of_trainable_model_parameters(peft_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:55:18.711054Z","iopub.execute_input":"2025-11-15T18:55:18.711305Z","iopub.status.idle":"2025-11-15T18:55:18.720220Z","shell.execute_reply.started":"2025-11-15T18:55:18.711287Z","shell.execute_reply":"2025-11-15T18:55:18.719578Z"}},"outputs":[{"name":"stdout","text":"Total Number of Parameters: 77649280\nTotal Number of Trainable Parameters: 0\n","output_type":"stream"}],"execution_count":95},{"cell_type":"markdown","source":"<a name='3.3'></a>\n### 3.3 - Evaluate the Model Qualitatively (Human Evaluation) (2 points)\n\nMake inferences for the same example as in sections [1.3](#1.3) and [2.3](#2.3), with the original model, fully fine-tuned and PEFT model.","metadata":{}},{"cell_type":"code","source":"# Get random dialogue and label from test dataset\nindex = 200\ndialogue = dataset['test'][index]['dialogue']\nhuman_baseline_summary = dataset['test'][index]['summary']\n\nprompt = \"Summarize the following conversation.\\n\\n\" + dialogue + \"\\n\\nSummary:\"\n\n# Tokenize the prompt\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n# Move input_ids to the same device as the model\ninput_ids = input_ids.to(device)\n\n# Get text output from original model\noriginal_model_response = original_model.generate(input_ids)\noriginal_model_outputs = original_model_response[0]\noriginal_model_text_output = original_tokenizer.decode(\n    original_model_outputs,\n    skip_special_tokens=True\n)\n\n# Get text output from instruct finetuned model\ninstruct_model_response = instruct_model.generate(input_ids)\ninstruct_model_outputs = instruct_model_response[0]\ninstruct_model_text_output = tokenizer.decode(\n    instruct_model_outputs,\n    skip_special_tokens=True\n)\n\n# Get text output from peft model\npeft_model_response = peft_model.generate(input_ids=input_ids)\npeft_model_outputs = peft_model_response[0]\npeft_model_text_output = peft_tokenizer.decode(\n    peft_model_outputs,\n    skip_special_tokens=True\n)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\nprint(dash_line)\nprint(f'PEFT MODEL: {peft_model_text_output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:55:27.679132Z","iopub.execute_input":"2025-11-15T18:55:27.679405Z","iopub.status.idle":"2025-11-15T18:55:28.739112Z","shell.execute_reply.started":"2025-11-15T18:55:27.679387Z","shell.execute_reply":"2025-11-15T18:55:28.738443Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\nORIGINAL MODEL:\n#Person1# thinks #Person2# should upgrade to the system because it is\n---------------------------------------------------------------------------------------------------\nINSTRUCT MODEL:\n#Person1# You could consider adding a painting program to your software.\n---------------------------------------------------------------------------------------------------\nPEFT MODEL: #Person1# thinks adding a painting program to the software would allow #Person\n","output_type":"stream"}],"execution_count":96},{"cell_type":"markdown","source":"3.3 Analyze the quality of summaries considering different aspects\n\nAs you can see in the output above, the finetuned model captures the conversation much better than the original model which ended up giving a different response to the test prompt this time. The original model doesn't really capture the purpose of the conversation well while the finetuned model at least captures some key points made in the conversation. In addition, the peft model surprisingly captures the conversation better than both the finetuned model and the original model. Unlike the finetuned model, the peft model is able to identify that the conversation is a  third-party conversation. Maybe the fully finetuned model was overfitting and the peft model is able to generalize much better than the fully finetuned model due to the reduction in model size.","metadata":{}},{"cell_type":"markdown","source":"<a name='3.4'></a>\n### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric) (3 points)\nPerform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time).","metadata":{}},{"cell_type":"code","source":"# Get 10 test set dialogues and their labels\ndialogues = dataset['test'][40:50]['dialogue']\nhuman_baseline_summaries = dataset['test'][40:50]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    prompt = \"Summarize the following conversation.\\n\\n\" + dialogue + \"\\n\\nSummary:\"\n\n    # Move input_ids to the same device as the model\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n    # Get text output from original model\n    original_model_response = original_model.generate(input_ids)\n    original_model_outputs = original_model_response[0]\n    original_model_text_output = original_tokenizer.decode(\n        original_model_outputs,\n        skip_special_tokens=True\n    )\n    \n    # Get text output from instruct finetuned model\n    instruct_model_response = instruct_model.generate(input_ids)\n    instruct_model_outputs = instruct_model_response[0]\n    instruct_model_text_output = instruct_tokenizer.decode(\n        instruct_model_outputs,\n        skip_special_tokens=True\n    )\n\n    # Get text output from peft model\n    peft_model_response = peft_model.generate(input_ids=input_ids)\n    peft_model_outputs = peft_model_response[0]\n    peft_model_text_output = peft_tokenizer.decode(\n        peft_model_outputs,\n        skip_special_tokens=True\n    )\n\n    # append the model outputs to their respective lists\n    original_model_summaries.append(original_model_text_output)\n    instruct_model_summaries.append(instruct_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n\n# display the outputs of all the models as a pandas dataframe\ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T19:28:52.659996Z","iopub.execute_input":"2025-11-15T19:28:52.660296Z","iopub.status.idle":"2025-11-15T19:29:02.523449Z","shell.execute_reply.started":"2025-11-15T19:28:52.660275Z","shell.execute_reply":"2025-11-15T19:29:02.522666Z"}},"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  #Person1# is in a hurry to catch a train. Tom ...   \n1  #Person1# is rushing to catch a train but Tom ...   \n2  #Person1# wants to adjust #Person1#'s life and...   \n3  #Person1# has a bad lifestyle. #Person2# kindl...   \n4  #Person2# hopes #Person1# will become healthy ...   \n5  #Person1# tells #Person2# that Ruojia is marri...   \n6  #Person2# is surprised to know from #Person1# ...   \n7  #Person2# is surprised that Ruojia's married. ...   \n8  #Person2# at first thinks #Person1#'s behaviou...   \n9  #Person1# plans on playing a trick to others. ...   \n\n                            original_model_summaries  \\\n0  #P1# tells Tom Tom is ten to nine by his watch...   \n1     #Person2# is off now and will catch the train.   \n2          #Person2# tells #Person1# #Person2# can't   \n3          #Person2# tells #Person2# #Person1# can't   \n4          #Person1# tells #Person1# #Person1# can't   \n5  #Person1# invites #Person2# to the party tonig...   \n6  Ruojia wants to go to the party tonight. #Pers...   \n7  #Person1# wants to go to the party. #Person1# ...   \n8       #Person1# tells #Person1# that #Person22# is   \n9      #Person1# tells #Person1# #Person1#'s friends   \n\n                            instruct_model_summaries  \\\n0  @Person, I'm not a fan of the ten to nine by m...   \n1  @Person, I'm not a fan of the ten to nine by m...   \n2                                          #Person1#   \n3                                          #Person1#   \n4                                          #Person1#   \n5       #Person! #Person! #Person! #Person! #Person!   \n6       #Person! #Person! #Person! #Person! #Person!   \n7       #Person! #Person! #Person! #Person! #Person!   \n8            #Person1# You might make a few enemies.   \n9            #Person1# You might make a few enemies.   \n\n                                peft_model_summaries  \n0  Tom is waiting for the train to arrive. He has...  \n1  Tom is waiting for the train to arrive. He has...  \n2      #Person1# tells #Person2# that #Person2# can'  \n3      #Person1# tells #Person2# that #Person2# can'  \n4      #Person1# tells #Person2# that #Person2# can'  \n5  Ruojia's party is going to be a party tonight....  \n6  Ruojia's party is going to be a party tonight....  \n7  Ruojia's party is going to be a party tonight....  \n8  #Person1# tells #Person2# that the two ugly ol...  \n9  #Person1# tells #Person2# that the two ugly ol...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>instruct_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#Person1# is in a hurry to catch a train. Tom ...</td>\n      <td>#P1# tells Tom Tom is ten to nine by his watch...</td>\n      <td>@Person, I'm not a fan of the ten to nine by m...</td>\n      <td>Tom is waiting for the train to arrive. He has...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#Person1# is rushing to catch a train but Tom ...</td>\n      <td>#Person2# is off now and will catch the train.</td>\n      <td>@Person, I'm not a fan of the ten to nine by m...</td>\n      <td>Tom is waiting for the train to arrive. He has...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#Person1# wants to adjust #Person1#'s life and...</td>\n      <td>#Person2# tells #Person1# #Person2# can't</td>\n      <td>#Person1#</td>\n      <td>#Person1# tells #Person2# that #Person2# can'</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person1# has a bad lifestyle. #Person2# kindl...</td>\n      <td>#Person2# tells #Person2# #Person1# can't</td>\n      <td>#Person1#</td>\n      <td>#Person1# tells #Person2# that #Person2# can'</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# hopes #Person1# will become healthy ...</td>\n      <td>#Person1# tells #Person1# #Person1# can't</td>\n      <td>#Person1#</td>\n      <td>#Person1# tells #Person2# that #Person2# can'</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person1# tells #Person2# that Ruojia is marri...</td>\n      <td>#Person1# invites #Person2# to the party tonig...</td>\n      <td>#Person! #Person! #Person! #Person! #Person!</td>\n      <td>Ruojia's party is going to be a party tonight....</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person2# is surprised to know from #Person1# ...</td>\n      <td>Ruojia wants to go to the party tonight. #Pers...</td>\n      <td>#Person! #Person! #Person! #Person! #Person!</td>\n      <td>Ruojia's party is going to be a party tonight....</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person2# is surprised that Ruojia's married. ...</td>\n      <td>#Person1# wants to go to the party. #Person1# ...</td>\n      <td>#Person! #Person! #Person! #Person! #Person!</td>\n      <td>Ruojia's party is going to be a party tonight....</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person2# at first thinks #Person1#'s behaviou...</td>\n      <td>#Person1# tells #Person1# that #Person22# is</td>\n      <td>#Person1# You might make a few enemies.</td>\n      <td>#Person1# tells #Person2# that the two ugly ol...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# plans on playing a trick to others. ...</td>\n      <td>#Person1# tells #Person1# #Person1#'s friends</td>\n      <td>#Person1# You might make a few enemies.</td>\n      <td>#Person1# tells #Person2# that the two ugly ol...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":104},{"cell_type":"markdown","source":"Compute ROUGE score for this subset of the data.","metadata":{}},{"cell_type":"code","source":"# Load the rouge evaluator\nrouge = evaluate.load('rouge')\n\n# Get rouge score metric for original model generated summaries\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries, references=human_baseline_summaries\n)\n\n# Get rouge score metric for finetuned model generated summaries\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries, references=human_baseline_summaries\n)\n\n# Get rouge score metric for peft model generated summaries\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries, references=human_baseline_summaries\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T19:29:10.238286Z","iopub.execute_input":"2025-11-15T19:29:10.239045Z","iopub.status.idle":"2025-11-15T19:29:11.125344Z","shell.execute_reply.started":"2025-11-15T19:29:10.239009Z","shell.execute_reply":"2025-11-15T19:29:11.124678Z"}},"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.2297754317843419, 'rouge2': 0.016856060606060607, 'rougeL': 0.19614187176847026, 'rougeLsum': 0.19629431610291276}\nINSTRUCT MODEL:\n{'rouge1': 0.11086793517633425, 'rouge2': 0.0, 'rougeL': 0.09812110581030985, 'rougeLsum': 0.09455990521235702}\nPEFT MODEL:\n{'rouge1': 0.27515248972164164, 'rouge2': 0.05506175640250051, 'rougeL': 0.20027310328222514, 'rougeLsum': 0.19921123063254126}\n","output_type":"stream"}],"execution_count":105},{"cell_type":"markdown","source":"3.4 Analyze PEFT vs. original model metrics Compare PEFT vs. full fine-tuning results\n\nAs you can clearly see from the output above, the finetuned model performed the worst, followed by the original model, and then the peft model which performed the best across all metrics. The fact that the finetuned model performed the worst indicates that finetuning the entire model caused overfitting. Surprisingly, the peft model performed the best which indicates that the model has way more parameters than it actually needs and reducing the model size can help it generalize better/produce better results on unseen data.","metadata":{}},{"cell_type":"markdown","source":"Notice, that PEFT model results are not too bad, while the training process was much easier!","metadata":{}},{"cell_type":"markdown","source":"You already computed ROUGE score on the full dataset, after loading the results from the `data/dialogue-summary-training-results.csv` file. Load the values for the PEFT model now and check its performance compared to other models.","metadata":{}},{"cell_type":"code","source":"# Update the path to the CSV file\nresults_path = \"../input/pre-populated-list/dialogue-summary-training-results.csv\"\nresults = pd.read_csv(results_path)\n\nhuman_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\npeft_model_summaries     = results['peft_model_summaries'].values\n\n# Get rouge score metric for original model generated summaries\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries, references=human_baseline_summaries\n)\n\n# Get rouge score metric for finetuned model generated summaries\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries, references=human_baseline_summaries\n)\n\n# Get rouge score metric for peft model generated summaries\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries, references=human_baseline_summaries\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T19:35:31.753134Z","iopub.execute_input":"2025-11-15T19:35:31.753710Z","iopub.status.idle":"2025-11-15T19:35:35.073994Z","shell.execute_reply.started":"2025-11-15T19:35:31.753687Z","shell.execute_reply":"2025-11-15T19:35:35.073287Z"}},"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.2216686882994889, 'rouge2': 0.0707492488737373, 'rougeL': 0.19245630286595683, 'rougeLsum': 0.192409231638204}\nINSTRUCT MODEL:\n{'rouge1': 0.4041959932817219, 'rouge2': 0.17064828985299663, 'rougeL': 0.3267557101191949, 'rougeLsum': 0.3266766725171105}\nPEFT MODEL:\n{'rouge1': 0.39119098357131776, 'rouge2': 0.15459808342905274, 'rougeL': 0.31367299251500014, 'rougeLsum': 0.31360615168633016}\n","output_type":"stream"}],"execution_count":107},{"cell_type":"markdown","source":"The results show less of an improvement over full fine-tuning, but the benefits of PEFT typically outweigh the slightly-lower performance metrics.\n\nCalculate the improvement of PEFT over the original model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T19:36:42.914813Z","iopub.execute_input":"2025-11-15T19:36:42.915518Z","iopub.status.idle":"2025-11-15T19:36:42.922260Z","shell.execute_reply.started":"2025-11-15T19:36:42.915495Z","shell.execute_reply":"2025-11-15T19:36:42.921344Z"}},"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 16.95%\nrouge2: 8.38%\nrougeL: 12.12%\nrougeLsum: 12.12%\n","output_type":"stream"}],"execution_count":108},{"cell_type":"markdown","source":"Now calculate the improvement of PEFT over a full fine-tuned model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T19:36:48.672312Z","iopub.execute_input":"2025-11-15T19:36:48.673059Z","iopub.status.idle":"2025-11-15T19:36:48.679610Z","shell.execute_reply.started":"2025-11-15T19:36:48.673030Z","shell.execute_reply":"2025-11-15T19:36:48.678834Z"}},"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\nrouge1: -1.30%\nrouge2: -1.61%\nrougeL: -1.31%\nrougeLsum: -1.31%\n","output_type":"stream"}],"execution_count":109},{"cell_type":"markdown","source":"3.4 Analyze and compare performance metrics between models\n\nEvaluate the trade-off between performance and computational efficiency\n\nAs you can clearly see from the output above, when evaluated across a larger section of data, the finetuned model beats both the original model and the peft model in every rouge metric. However, the peft model rouge metrics are very close to the finetuned model metrics indicating that reducing the model size with LoRA still allows it to produce comparable results that are pretty close to what we can achieve with full model finetuning. Finetuning the peft model is also way more computationally efficient due to the significantly less amount of parameters that it handles as shown in the previous output that prints the number of trainable parameters for the peft model. As a result, it might be worth it to just finetune the peft model for more epochs than finetuning the full model if there's a computational budget to be met. In this case the peft model might actually produce better results than finetuning the full model.","metadata":{}},{"cell_type":"markdown","source":"Here you see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. However, the training requires much less computing and memory resources (often just a single GPU).","metadata":{}},{"cell_type":"markdown","source":"Limitations Encountered During the Fine-Tuning Process: I was not really able to get good performance when finetuning the full model here on Kaggle no matter what reasonable values I tried for number of epochs(1-5) and learning rates I tried. I settled on this after around 2 hours of trying to figure this out.\n\nI've executed this entire notebook on Kaggle where some of the necessary dependencies and libraries are already installed and don't need to be explicitly installed using pip install.","metadata":{}}]}